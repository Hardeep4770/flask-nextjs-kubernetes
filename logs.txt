* 
* ==> Audit <==
* |--------------|------------------------|----------|---------|---------|---------------------|---------------------|
|   Command    |          Args          | Profile  |  User   | Version |     Start Time      |      End Time       |
|--------------|------------------------|----------|---------|---------|---------------------|---------------------|
| dashboard    |                        | minikube | hardeep | v1.27.0 | 13 Oct 22 13:30 IST |                     |
| ip           |                        | minikube | hardeep | v1.27.0 | 13 Oct 22 13:31 IST | 13 Oct 22 13:31 IST |
| start        | --driver=docker        | minikube | hardeep | v1.27.0 | 13 Oct 22 14:43 IST |                     |
| delete       |                        | minikube | hardeep | v1.27.0 | 13 Oct 22 14:46 IST |                     |
| update-check |                        | minikube | hardeep | v1.27.1 | 13 Oct 22 14:55 IST | 13 Oct 22 14:55 IST |
| delete       |                        | minikube | hardeep | v1.27.0 | 14 Oct 22 13:04 IST | 14 Oct 22 13:04 IST |
| start        | --driver=docker        | minikube | hardeep | v1.27.0 | 14 Oct 22 14:56 IST | 14 Oct 22 14:57 IST |
| service      | flask-service --url    | minikube | hardeep | v1.27.0 | 14 Oct 22 15:07 IST | 14 Oct 22 15:08 IST |
| service      | flask-service --url    | minikube | hardeep | v1.27.0 | 14 Oct 22 15:25 IST | 14 Oct 22 15:26 IST |
| ip           |                        | minikube | hardeep | v1.27.0 | 14 Oct 22 15:39 IST | 14 Oct 22 15:39 IST |
| ip           |                        | minikube | hardeep | v1.27.0 | 14 Oct 22 15:44 IST | 14 Oct 22 15:44 IST |
| addons       | enable ingress         | minikube | hardeep | v1.27.0 | 14 Oct 22 15:48 IST | 14 Oct 22 15:49 IST |
| addons       | enable ingress         | minikube | hardeep | v1.27.0 | 14 Oct 22 15:49 IST | 14 Oct 22 15:49 IST |
| addons       | enable ingress         | minikube | hardeep | v1.27.0 | 14 Oct 22 15:49 IST | 14 Oct 22 15:49 IST |
| addons       | enable ingress         | minikube | hardeep | v1.27.0 | 14 Oct 22 15:52 IST | 14 Oct 22 15:52 IST |
| addons       | enable ingress         | minikube | hardeep | v1.27.0 | 14 Oct 22 15:53 IST | 14 Oct 22 15:53 IST |
| addons       | enable ingress         | minikube | hardeep | v1.27.0 | 14 Oct 22 15:57 IST | 14 Oct 22 15:57 IST |
| addons       | enable ingress         | minikube | hardeep | v1.27.0 | 14 Oct 22 15:58 IST | 14 Oct 22 15:58 IST |
| addons       | enable ingress         | minikube | hardeep | v1.27.0 | 14 Oct 22 16:00 IST | 14 Oct 22 16:00 IST |
| dashboard    |                        | minikube | hardeep | v1.27.0 | 14 Oct 22 16:00 IST |                     |
| dashboard    |                        | minikube | hardeep | v1.27.0 | 14 Oct 22 16:01 IST |                     |
| dashboard    |                        | minikube | hardeep | v1.27.0 | 14 Oct 22 16:01 IST |                     |
| addons       | enable ingress         | minikube | hardeep | v1.27.0 | 14 Oct 22 16:03 IST | 14 Oct 22 16:03 IST |
| addons       | enable ingress         | minikube | hardeep | v1.27.0 | 14 Oct 22 16:10 IST | 14 Oct 22 16:10 IST |
| addons       | enable ingress         | minikube | hardeep | v1.27.0 | 14 Oct 22 16:11 IST | 14 Oct 22 16:11 IST |
| addons       | enable ingress         | minikube | hardeep | v1.27.0 | 14 Oct 22 16:56 IST | 14 Oct 22 16:56 IST |
| addons       | enable ingress         | minikube | hardeep | v1.27.0 | 14 Oct 22 16:57 IST | 14 Oct 22 16:57 IST |
| addons       | enable ingress         | minikube | hardeep | v1.27.0 | 14 Oct 22 16:58 IST | 14 Oct 22 16:58 IST |
| addons       | enable ingress         | minikube | hardeep | v1.27.0 | 14 Oct 22 16:58 IST | 14 Oct 22 16:58 IST |
| addons       | enable ingress         | minikube | hardeep | v1.27.0 | 14 Oct 22 16:59 IST | 14 Oct 22 17:00 IST |
| addons       | enable ingress         | minikube | hardeep | v1.27.0 | 14 Oct 22 17:01 IST | 14 Oct 22 17:01 IST |
| delete       |                        | minikube | hardeep | v1.27.0 | 14 Oct 22 18:01 IST | 14 Oct 22 18:01 IST |
| start        | --driver=docker        | minikube | hardeep | v1.27.0 | 14 Oct 22 18:18 IST | 14 Oct 22 18:19 IST |
| addons       | enable ingress         | minikube | hardeep | v1.27.0 | 14 Oct 22 18:21 IST |                     |
| addons       | enable ingress         | minikube | hardeep | v1.27.0 | 14 Oct 22 18:23 IST | 14 Oct 22 18:25 IST |
| ip           |                        | minikube | hardeep | v1.27.0 | 14 Oct 22 18:33 IST | 14 Oct 22 18:33 IST |
| addons       | enable ingress         | minikube | hardeep | v1.27.0 | 14 Oct 22 18:34 IST | 14 Oct 22 18:34 IST |
| addons       | enable ingress         | minikube | hardeep | v1.27.0 | 14 Oct 22 18:35 IST | 14 Oct 22 18:35 IST |
| addons       | enable ingress         | minikube | hardeep | v1.27.0 | 14 Oct 22 18:37 IST | 14 Oct 22 18:37 IST |
| addons       | enable ingress         | minikube | hardeep | v1.27.0 | 14 Oct 22 18:39 IST | 14 Oct 22 18:39 IST |
| addons       | enable ingress         | minikube | hardeep | v1.27.0 | 14 Oct 22 18:43 IST | 14 Oct 22 18:43 IST |
| addons       | enable ingress         | minikube | hardeep | v1.27.0 | 14 Oct 22 18:52 IST | 14 Oct 22 18:52 IST |
| stop         |                        | minikube | hardeep | v1.27.0 | 14 Oct 22 19:07 IST | 14 Oct 22 19:07 IST |
| delete       |                        | minikube | hardeep | v1.27.0 | 14 Oct 22 19:08 IST | 14 Oct 22 19:08 IST |
| start        | --driver=docker        | minikube | hardeep | v1.27.0 | 14 Oct 22 19:08 IST | 14 Oct 22 19:09 IST |
| addons       | enable ingress         | minikube | hardeep | v1.27.0 | 14 Oct 22 19:10 IST |                     |
| addons       | enable ingress         | minikube | hardeep | v1.27.0 | 14 Oct 22 19:14 IST | 14 Oct 22 19:15 IST |
| service      | frontend-service --url | minikube | hardeep | v1.27.0 | 14 Oct 22 19:16 IST |                     |
| update-check |                        | minikube | hardeep | v1.27.1 | 17 Oct 22 09:40 IST | 17 Oct 22 09:40 IST |
| update-check |                        | minikube | hardeep | v1.27.1 | 18 Oct 22 15:05 IST | 18 Oct 22 15:05 IST |
| delete       |                        | minikube | hardeep | v1.27.0 | 18 Oct 22 17:22 IST | 18 Oct 22 17:22 IST |
| start        | --driver=docker        | minikube | hardeep | v1.27.0 | 18 Oct 22 18:28 IST | 18 Oct 22 18:29 IST |
| addons       | enable ingress         | minikube | hardeep | v1.27.0 | 18 Oct 22 18:30 IST | 18 Oct 22 18:35 IST |
| service      | frontend-service --url | minikube | hardeep | v1.27.0 | 18 Oct 22 18:36 IST |                     |
| update-check |                        | minikube | hardeep | v1.27.1 | 19 Oct 22 09:43 IST | 19 Oct 22 09:43 IST |
| delete       |                        | minikube | hardeep | v1.27.0 | 19 Oct 22 10:11 IST | 19 Oct 22 10:11 IST |
| start        | --driver=docker        | minikube | hardeep | v1.27.0 | 19 Oct 22 10:23 IST | 19 Oct 22 10:24 IST |
| addons       | enable ingress         | minikube | hardeep | v1.27.0 | 19 Oct 22 10:24 IST | 19 Oct 22 10:27 IST |
| service      | frontend-service --url | minikube | hardeep | v1.27.0 | 19 Oct 22 10:38 IST | 19 Oct 22 10:39 IST |
| ip           |                        | minikube | hardeep | v1.27.0 | 19 Oct 22 10:39 IST | 19 Oct 22 10:39 IST |
|--------------|------------------------|----------|---------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2022/10/19 10:23:14
Running on machine: Harvey-Hardeep
Binary: Built with gc go1.19.1 for darwin/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1019 10:23:14.875783   31786 out.go:296] Setting OutFile to fd 1 ...
I1019 10:23:14.876384   31786 out.go:348] isatty.IsTerminal(1) = true
I1019 10:23:14.876390   31786 out.go:309] Setting ErrFile to fd 2...
I1019 10:23:14.876400   31786 out.go:348] isatty.IsTerminal(2) = true
I1019 10:23:14.876582   31786 root.go:333] Updating PATH: /Users/hardeep/.minikube/bin
I1019 10:23:14.879518   31786 out.go:303] Setting JSON to false
I1019 10:23:14.908975   31786 start.go:115] hostinfo: {"hostname":"Harvey-Hardeep.local","uptime":2557,"bootTime":1666152637,"procs":515,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"12.4","kernelVersion":"21.5.0","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"d921bc88-306d-5b5f-b976-c8d9519fea38"}
W1019 10:23:14.909087   31786 start.go:123] gopshost.Virtualization returned error: not implemented yet
I1019 10:23:14.914137   31786 out.go:177] üòÑ  minikube v1.27.0 on Darwin 12.4
W1019 10:23:14.917609   31786 out.go:239] ‚ùó  Kubernetes 1.25.0 has a known issue with resolv.conf. minikube is using a workaround that should work for most use cases.
W1019 10:23:14.917665   31786 out.go:239] ‚ùó  For more information, see: https://github.com/kubernetes/kubernetes/issues/112135
I1019 10:23:14.918608   31786 notify.go:214] Checking for updates...
I1019 10:23:14.919912   31786 driver.go:365] Setting default libvirt URI to qemu:///system
I1019 10:23:15.331532   31786 docker.go:137] docker version: linux-20.10.17
I1019 10:23:15.331677   31786 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1019 10:23:16.238446   31786 info.go:265] docker info: {ID:NZYT:GIQJ:EKHI:6XAY:CYDA:JA5I:TWGH:LPB2:YBJE:J6YI:4UGS:A36E Containers:3 ContainersRunning:1 ContainersPaused:0 ContainersStopped:2 Images:5 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:48 OomKillDisable:false NGoroutines:51 SystemTime:2022-10-19 04:53:15.428069524 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:5 KernelVersion:5.10.124-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:2 MemTotal:4125900800 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.17 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:9cd3357b7fd7218e4aec3eae239db1f68a5a6ec6 Expected:9cd3357b7fd7218e4aec3eae239db1f68a5a6ec6} RuncCommit:{ID:v1.1.4-0-g5fd4c4d Expected:v1.1.4-0-g5fd4c4d} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.9.1] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.10.2] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.9] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/usr/local/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.19.0]] Warnings:<nil>}}
I1019 10:23:16.246360   31786 out.go:177] ‚ú®  Using the docker driver based on user configuration
I1019 10:23:16.250318   31786 start.go:284] selected driver: docker
I1019 10:23:16.250330   31786 start.go:808] validating driver "docker" against <nil>
I1019 10:23:16.250346   31786 start.go:819] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1019 10:23:16.250486   31786 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1019 10:23:16.467988   31786 info.go:265] docker info: {ID:NZYT:GIQJ:EKHI:6XAY:CYDA:JA5I:TWGH:LPB2:YBJE:J6YI:4UGS:A36E Containers:3 ContainersRunning:1 ContainersPaused:0 ContainersStopped:2 Images:5 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:48 OomKillDisable:false NGoroutines:51 SystemTime:2022-10-19 04:53:16.360704285 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:5 KernelVersion:5.10.124-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:2 MemTotal:4125900800 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.17 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:9cd3357b7fd7218e4aec3eae239db1f68a5a6ec6 Expected:9cd3357b7fd7218e4aec3eae239db1f68a5a6ec6} RuncCommit:{ID:v1.1.4-0-g5fd4c4d Expected:v1.1.4-0-g5fd4c4d} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.9.1] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.10.2] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.9] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/usr/local/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.19.0]] Warnings:<nil>}}
I1019 10:23:16.468146   31786 start_flags.go:296] no existing cluster config was found, will generate one from the flags 
I1019 10:23:16.468378   31786 start_flags.go:377] Using suggested 2200MB memory alloc based on sys=8192MB, container=3934MB
I1019 10:23:16.468473   31786 start_flags.go:835] Wait components to verify : map[apiserver:true system_pods:true]
I1019 10:23:16.474052   31786 out.go:177] üìå  Using Docker Desktop driver with root privileges
I1019 10:23:16.477609   31786 cni.go:95] Creating CNI manager for ""
I1019 10:23:16.477624   31786 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I1019 10:23:16.477634   31786 start_flags.go:310] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.25.0 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath:}
I1019 10:23:16.481457   31786 out.go:177] üëç  Starting control plane node minikube in cluster minikube
I1019 10:23:16.488506   31786 cache.go:120] Beginning downloading kic base image for docker with docker
I1019 10:23:16.492299   31786 out.go:177] üöú  Pulling base image ...
I1019 10:23:16.502805   31786 preload.go:132] Checking if preload exists for k8s version v1.25.0 and runtime docker
I1019 10:23:16.502969   31786 image.go:75] Checking for gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c in local docker daemon
I1019 10:23:16.503709   31786 preload.go:148] Found local preload: /Users/hardeep/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.25.0-docker-overlay2-amd64.tar.lz4
I1019 10:23:16.503784   31786 cache.go:57] Caching tarball of preloaded images
I1019 10:23:16.504530   31786 preload.go:174] Found /Users/hardeep/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.25.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1019 10:23:16.504543   31786 cache.go:60] Finished verifying existence of preloaded tar for  v1.25.0 on docker
I1019 10:23:16.505124   31786 profile.go:148] Saving config to /Users/hardeep/.minikube/profiles/minikube/config.json ...
I1019 10:23:16.505155   31786 lock.go:35] WriteFile acquiring /Users/hardeep/.minikube/profiles/minikube/config.json: {Name:mkc8a8cf8b5d8d5524820ac23ccbf7cd94b20371 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1019 10:23:16.616377   31786 image.go:79] Found gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c in local docker daemon, skipping pull
I1019 10:23:16.616680   31786 cache.go:142] gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c exists in daemon, skipping load
I1019 10:23:16.616700   31786 cache.go:208] Successfully downloaded all kic artifacts
I1019 10:23:16.616781   31786 start.go:364] acquiring machines lock for minikube: {Name:mk2475592beb3166493c61b732e53d72c2faff02 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1019 10:23:16.616972   31786 start.go:368] acquired machines lock for "minikube" in 178.091¬µs
I1019 10:23:16.617318   31786 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.25.0 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.25.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath:} &{Name: IP: Port:8443 KubernetesVersion:v1.25.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1019 10:23:16.617406   31786 start.go:125] createHost starting for "" (driver="docker")
I1019 10:23:16.622776   31786 out.go:204] üî•  Creating docker container (CPUs=2, Memory=2200MB) ...
I1019 10:23:16.623734   31786 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I1019 10:23:16.623780   31786 client.go:168] LocalClient.Create starting
I1019 10:23:16.624212   31786 main.go:134] libmachine: Reading certificate data from /Users/hardeep/.minikube/certs/ca.pem
I1019 10:23:16.624566   31786 main.go:134] libmachine: Decoding PEM data...
I1019 10:23:16.624591   31786 main.go:134] libmachine: Parsing certificate...
I1019 10:23:16.624939   31786 main.go:134] libmachine: Reading certificate data from /Users/hardeep/.minikube/certs/cert.pem
I1019 10:23:16.625215   31786 main.go:134] libmachine: Decoding PEM data...
I1019 10:23:16.625229   31786 main.go:134] libmachine: Parsing certificate...
I1019 10:23:16.625790   31786 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W1019 10:23:16.728465   31786 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I1019 10:23:16.728580   31786 network_create.go:272] running [docker network inspect minikube] to gather additional debugging logs...
I1019 10:23:16.728609   31786 cli_runner.go:164] Run: docker network inspect minikube
W1019 10:23:16.822404   31786 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I1019 10:23:16.822430   31786 network_create.go:275] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error: No such network: minikube
I1019 10:23:16.822443   31786 network_create.go:277] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error: No such network: minikube

** /stderr **
I1019 10:23:16.822549   31786 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1019 10:23:16.923785   31786 network.go:290] reserving subnet 192.168.49.0 for 1m0s: &{mu:{state:0 sema:0} read:{v:{m:map[] amended:true}} dirty:map[192.168.49.0:0xc000156090] misses:0}
I1019 10:23:16.923825   31786 network.go:236] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:}}
I1019 10:23:16.923842   31786 network_create.go:115] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I1019 10:23:16.923948   31786 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I1019 10:23:17.152203   31786 network_create.go:99] docker network minikube 192.168.49.0/24 created
I1019 10:23:17.152231   31786 kic.go:106] calculated static IP "192.168.49.2" for the "minikube" container
I1019 10:23:17.152359   31786 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I1019 10:23:17.257897   31786 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I1019 10:23:17.367933   31786 oci.go:103] Successfully created a docker volume minikube
I1019 10:23:17.368098   31786 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c -d /var/lib
I1019 10:23:19.323216   31786 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c -d /var/lib: (1.955066728s)
I1019 10:23:19.323237   31786 oci.go:107] Successfully prepared a docker volume minikube
I1019 10:23:19.323267   31786 preload.go:132] Checking if preload exists for k8s version v1.25.0 and runtime docker
I1019 10:23:19.323284   31786 kic.go:179] Starting extracting preloaded images to volume ...
I1019 10:23:19.323407   31786 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /Users/hardeep/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.25.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c -I lz4 -xf /preloaded.tar -C /extractDir
I1019 10:23:37.327200   31786 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /Users/hardeep/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.25.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c -I lz4 -xf /preloaded.tar -C /extractDir: (18.00361574s)
I1019 10:23:37.327315   31786 kic.go:188] duration metric: took 18.004075 seconds to extract preloaded images to volume
I1019 10:23:37.328626   31786 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I1019 10:23:38.303423   31786 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=2200mb --memory-swap=2200mb --cpus=2 -e container=docker --expose 8443 --publish=8443 --publish=22 --publish=2376 --publish=5000 --publish=32443 gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c
I1019 10:23:39.143650   31786 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I1019 10:23:39.364169   31786 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1019 10:23:39.595334   31786 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I1019 10:23:39.806808   31786 oci.go:144] the created container "minikube" has a running status.
I1019 10:23:39.807227   31786 kic.go:210] Creating ssh key for kic: /Users/hardeep/.minikube/machines/minikube/id_rsa...
I1019 10:23:40.013769   31786 kic_runner.go:191] docker (temp): /Users/hardeep/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I1019 10:23:40.278338   31786 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1019 10:23:40.388483   31786 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I1019 10:23:40.388499   31786 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I1019 10:23:40.569969   31786 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1019 10:23:40.672264   31786 machine.go:88] provisioning docker machine ...
I1019 10:23:40.673356   31786 ubuntu.go:169] provisioning hostname "minikube"
I1019 10:23:40.673631   31786 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1019 10:23:40.773935   31786 main.go:134] libmachine: Using SSH client type: native
I1019 10:23:40.775287   31786 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x43ec000] 0x43ef180 <nil>  [] 0s} 127.0.0.1 54843 <nil> <nil>}
I1019 10:23:40.775315   31786 main.go:134] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1019 10:23:40.820530   31786 main.go:134] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I1019 10:23:44.029684   31786 main.go:134] libmachine: SSH cmd err, output: <nil>: minikube

I1019 10:23:44.030061   31786 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1019 10:23:44.121961   31786 main.go:134] libmachine: Using SSH client type: native
I1019 10:23:44.122156   31786 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x43ec000] 0x43ef180 <nil>  [] 0s} 127.0.0.1 54843 <nil> <nil>}
I1019 10:23:44.122167   31786 main.go:134] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1019 10:23:44.283197   31786 main.go:134] libmachine: SSH cmd err, output: <nil>: 
I1019 10:23:44.283825   31786 ubuntu.go:175] set auth options {CertDir:/Users/hardeep/.minikube CaCertPath:/Users/hardeep/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/hardeep/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/hardeep/.minikube/machines/server.pem ServerKeyPath:/Users/hardeep/.minikube/machines/server-key.pem ClientKeyPath:/Users/hardeep/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/hardeep/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/hardeep/.minikube}
I1019 10:23:44.283861   31786 ubuntu.go:177] setting up certificates
I1019 10:23:44.283871   31786 provision.go:83] configureAuth start
I1019 10:23:44.283986   31786 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1019 10:23:44.374018   31786 provision.go:138] copyHostCerts
I1019 10:23:44.374152   31786 exec_runner.go:144] found /Users/hardeep/.minikube/ca.pem, removing ...
I1019 10:23:44.374159   31786 exec_runner.go:207] rm: /Users/hardeep/.minikube/ca.pem
I1019 10:23:44.374284   31786 exec_runner.go:151] cp: /Users/hardeep/.minikube/certs/ca.pem --> /Users/hardeep/.minikube/ca.pem (1078 bytes)
I1019 10:23:44.376608   31786 exec_runner.go:144] found /Users/hardeep/.minikube/cert.pem, removing ...
I1019 10:23:44.376621   31786 exec_runner.go:207] rm: /Users/hardeep/.minikube/cert.pem
I1019 10:23:44.376733   31786 exec_runner.go:151] cp: /Users/hardeep/.minikube/certs/cert.pem --> /Users/hardeep/.minikube/cert.pem (1123 bytes)
I1019 10:23:44.377266   31786 exec_runner.go:144] found /Users/hardeep/.minikube/key.pem, removing ...
I1019 10:23:44.377271   31786 exec_runner.go:207] rm: /Users/hardeep/.minikube/key.pem
I1019 10:23:44.377445   31786 exec_runner.go:151] cp: /Users/hardeep/.minikube/certs/key.pem --> /Users/hardeep/.minikube/key.pem (1679 bytes)
I1019 10:23:44.379844   31786 provision.go:112] generating server cert: /Users/hardeep/.minikube/machines/server.pem ca-key=/Users/hardeep/.minikube/certs/ca.pem private-key=/Users/hardeep/.minikube/certs/ca-key.pem org=hardeep.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I1019 10:23:44.448446   31786 provision.go:172] copyRemoteCerts
I1019 10:23:44.449022   31786 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1019 10:23:44.449119   31786 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1019 10:23:44.547616   31786 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54843 SSHKeyPath:/Users/hardeep/.minikube/machines/minikube/id_rsa Username:docker}
I1019 10:23:44.662017   31786 ssh_runner.go:362] scp /Users/hardeep/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1078 bytes)
I1019 10:23:44.696492   31786 ssh_runner.go:362] scp /Users/hardeep/.minikube/machines/server.pem --> /etc/docker/server.pem (1204 bytes)
I1019 10:23:44.730906   31786 ssh_runner.go:362] scp /Users/hardeep/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I1019 10:23:44.761800   31786 provision.go:86] duration metric: configureAuth took 477.399727ms
I1019 10:23:44.761812   31786 ubuntu.go:193] setting minikube options for container-runtime
I1019 10:23:44.762242   31786 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.25.0
I1019 10:23:44.762305   31786 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1019 10:23:45.093431   31786 main.go:134] libmachine: Using SSH client type: native
I1019 10:23:45.093593   31786 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x43ec000] 0x43ef180 <nil>  [] 0s} 127.0.0.1 54843 <nil> <nil>}
I1019 10:23:45.093599   31786 main.go:134] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1019 10:23:45.286027   31786 main.go:134] libmachine: SSH cmd err, output: <nil>: overlay

I1019 10:23:45.286037   31786 ubuntu.go:71] root file system type: overlay
I1019 10:23:45.286199   31786 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I1019 10:23:45.286294   31786 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1019 10:23:45.438536   31786 main.go:134] libmachine: Using SSH client type: native
I1019 10:23:45.439426   31786 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x43ec000] 0x43ef180 <nil>  [] 0s} 127.0.0.1 54843 <nil> <nil>}
I1019 10:23:45.439488   31786 main.go:134] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1019 10:23:45.639866   31786 main.go:134] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1019 10:23:45.640222   31786 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1019 10:23:45.796464   31786 main.go:134] libmachine: Using SSH client type: native
I1019 10:23:45.796673   31786 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x43ec000] 0x43ef180 <nil>  [] 0s} 127.0.0.1 54843 <nil> <nil>}
I1019 10:23:45.796685   31786 main.go:134] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1019 10:23:46.805482   31786 main.go:134] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2022-06-06 23:01:03.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2022-10-19 04:53:45.648242847 +0000
@@ -1,30 +1,32 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
 Wants=network-online.target
-Requires=docker.socket containerd.service
+Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutSec=0
-RestartSec=2
-Restart=always
-
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
+Restart=on-failure
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
@@ -32,16 +34,16 @@
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I1019 10:23:46.805503   31786 machine.go:91] provisioned docker machine in 6.133236544s
I1019 10:23:46.805510   31786 client.go:171] LocalClient.Create took 30.181801298s
I1019 10:23:46.805528   31786 start.go:167] duration metric: libmachine.API.Create for "minikube" took 30.181868778s
I1019 10:23:46.805842   31786 start.go:300] post-start starting for "minikube" (driver="docker")
I1019 10:23:46.805849   31786 start.go:328] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1019 10:23:46.805993   31786 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1019 10:23:46.806047   31786 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1019 10:23:46.911381   31786 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54843 SSHKeyPath:/Users/hardeep/.minikube/machines/minikube/id_rsa Username:docker}
I1019 10:23:47.032025   31786 ssh_runner.go:195] Run: cat /etc/os-release
I1019 10:23:47.042315   31786 main.go:134] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1019 10:23:47.042330   31786 main.go:134] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1019 10:23:47.042353   31786 main.go:134] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1019 10:23:47.042623   31786 info.go:137] Remote host: Ubuntu 20.04.5 LTS
I1019 10:23:47.042816   31786 filesync.go:126] Scanning /Users/hardeep/.minikube/addons for local assets ...
I1019 10:23:47.042946   31786 filesync.go:126] Scanning /Users/hardeep/.minikube/files for local assets ...
I1019 10:23:47.042999   31786 start.go:303] post-start completed in 237.150957ms
I1019 10:23:47.043694   31786 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1019 10:23:47.142509   31786 profile.go:148] Saving config to /Users/hardeep/.minikube/profiles/minikube/config.json ...
I1019 10:23:47.143165   31786 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1019 10:23:47.143275   31786 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1019 10:23:47.236189   31786 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54843 SSHKeyPath:/Users/hardeep/.minikube/machines/minikube/id_rsa Username:docker}
I1019 10:23:47.344680   31786 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1019 10:23:47.354541   31786 start.go:128] duration metric: createHost completed in 30.737198394s
I1019 10:23:47.354560   31786 start.go:83] releasing machines lock for "minikube", held for 30.737656618s
I1019 10:23:47.354661   31786 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1019 10:23:47.448751   31786 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1019 10:23:47.448773   31786 ssh_runner.go:195] Run: systemctl --version
I1019 10:23:47.448847   31786 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1019 10:23:47.449952   31786 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1019 10:23:47.696010   31786 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54843 SSHKeyPath:/Users/hardeep/.minikube/machines/minikube/id_rsa Username:docker}
I1019 10:23:47.726493   31786 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54843 SSHKeyPath:/Users/hardeep/.minikube/machines/minikube/id_rsa Username:docker}
I1019 10:23:48.667281   31786 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (1.218509413s)
I1019 10:23:48.667289   31786 ssh_runner.go:235] Completed: systemctl --version: (1.218508255s)
I1019 10:23:48.667838   31786 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1019 10:23:48.686341   31786 cruntime.go:273] skipping containerd shutdown because we are bound to it
I1019 10:23:48.686656   31786 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1019 10:23:48.701977   31786 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
image-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1019 10:23:48.727120   31786 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1019 10:23:48.894234   31786 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1019 10:23:49.002414   31786 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1019 10:23:49.117559   31786 ssh_runner.go:195] Run: sudo systemctl restart docker
I1019 10:23:49.594982   31786 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1019 10:23:49.706267   31786 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1019 10:23:49.803817   31786 ssh_runner.go:195] Run: sudo systemctl start cri-docker.socket
I1019 10:23:49.825690   31786 start.go:450] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1019 10:23:49.826755   31786 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1019 10:23:49.832719   31786 start.go:471] Will wait 60s for crictl version
I1019 10:23:49.832822   31786 ssh_runner.go:195] Run: sudo crictl version
I1019 10:23:50.343556   31786 start.go:480] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  20.10.17
RuntimeApiVersion:  1.41.0
I1019 10:23:50.343643   31786 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1019 10:23:50.551616   31786 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1019 10:23:50.614318   31786 out.go:204] üê≥  Preparing Kubernetes v1.25.0 on Docker 20.10.17 ...
I1019 10:23:50.614695   31786 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I1019 10:23:50.858184   31786 network.go:96] got host ip for mount in container by digging dns: 192.168.65.2
I1019 10:23:50.858500   31786 ssh_runner.go:195] Run: grep 192.168.65.2	host.minikube.internal$ /etc/hosts
I1019 10:23:50.867003   31786 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.2	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1019 10:23:50.882176   31786 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1019 10:23:50.976744   31786 preload.go:132] Checking if preload exists for k8s version v1.25.0 and runtime docker
I1019 10:23:50.976870   31786 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1019 10:23:51.023723   31786 docker.go:611] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.25.0
registry.k8s.io/kube-controller-manager:v1.25.0
registry.k8s.io/kube-scheduler:v1.25.0
registry.k8s.io/kube-proxy:v1.25.0
registry.k8s.io/pause:3.8
registry.k8s.io/etcd:3.5.4-0
registry.k8s.io/coredns/coredns:v1.9.3
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1019 10:23:51.023739   31786 docker.go:542] Images already preloaded, skipping extraction
I1019 10:23:51.023808   31786 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1019 10:23:51.073780   31786 docker.go:611] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.25.0
registry.k8s.io/kube-scheduler:v1.25.0
registry.k8s.io/kube-controller-manager:v1.25.0
registry.k8s.io/kube-proxy:v1.25.0
registry.k8s.io/pause:3.8
registry.k8s.io/etcd:3.5.4-0
registry.k8s.io/coredns/coredns:v1.9.3
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1019 10:23:51.073797   31786 cache_images.go:84] Images are preloaded, skipping loading
I1019 10:23:51.074161   31786 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1019 10:23:51.465599   31786 cni.go:95] Creating CNI manager for ""
I1019 10:23:51.465611   31786 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I1019 10:23:51.466662   31786 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I1019 10:23:51.466687   31786 kubeadm.go:156] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.25.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:true}
I1019 10:23:51.467077   31786 kubeadm.go:161] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: /var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.25.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
resolvConf: /etc/kubelet-resolv.conf
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1019 10:23:51.467909   31786 kubeadm.go:962] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.25.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=remote --container-runtime-endpoint=/var/run/cri-dockerd.sock --hostname-override=minikube --image-service-endpoint=/var/run/cri-dockerd.sock --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2 --runtime-request-timeout=15m

[Install]
 config:
{KubernetesVersion:v1.25.0 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I1019 10:23:51.468019   31786 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.25.0
I1019 10:23:51.482652   31786 binaries.go:44] Found k8s binaries, skipping transfer
I1019 10:23:51.482770   31786 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1019 10:23:51.492704   31786 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (470 bytes)
I1019 10:23:51.513424   31786 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1019 10:23:51.532950   31786 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2067 bytes)
I1019 10:23:51.552069   31786 ssh_runner.go:195] Run: sudo cp /etc/resolv.conf /etc/kubelet-resolv.conf
I1019 10:23:51.566540   31786 ssh_runner.go:195] Run: sudo sed -i -e "s/^search .$//" /etc/kubelet-resolv.conf
I1019 10:23:51.577963   31786 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1019 10:23:51.583930   31786 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1019 10:23:51.599211   31786 certs.go:54] Setting up /Users/hardeep/.minikube/profiles/minikube for IP: 192.168.49.2
I1019 10:23:51.599585   31786 certs.go:182] skipping minikubeCA CA generation: /Users/hardeep/.minikube/ca.key
I1019 10:23:51.599893   31786 certs.go:182] skipping proxyClientCA CA generation: /Users/hardeep/.minikube/proxy-client-ca.key
I1019 10:23:51.599943   31786 certs.go:302] generating minikube-user signed cert: /Users/hardeep/.minikube/profiles/minikube/client.key
I1019 10:23:51.600335   31786 crypto.go:68] Generating cert /Users/hardeep/.minikube/profiles/minikube/client.crt with IP's: []
I1019 10:23:51.733734   31786 crypto.go:156] Writing cert to /Users/hardeep/.minikube/profiles/minikube/client.crt ...
I1019 10:23:51.733753   31786 lock.go:35] WriteFile acquiring /Users/hardeep/.minikube/profiles/minikube/client.crt: {Name:mkf2764f67c98716fe3d2451c8faf091285de300 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1019 10:23:51.734280   31786 crypto.go:164] Writing key to /Users/hardeep/.minikube/profiles/minikube/client.key ...
I1019 10:23:51.734296   31786 lock.go:35] WriteFile acquiring /Users/hardeep/.minikube/profiles/minikube/client.key: {Name:mk5988da5e12820dd1d65de406825ac7b61dcc81 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1019 10:23:51.734689   31786 certs.go:302] generating minikube signed cert: /Users/hardeep/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I1019 10:23:51.734712   31786 crypto.go:68] Generating cert /Users/hardeep/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2 with IP's: [192.168.49.2 10.96.0.1 127.0.0.1 10.0.0.1]
I1019 10:23:52.082403   31786 crypto.go:156] Writing cert to /Users/hardeep/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2 ...
I1019 10:23:52.082421   31786 lock.go:35] WriteFile acquiring /Users/hardeep/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2: {Name:mk8744279543ee86cc291087cfc89dbd66c43cff Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1019 10:23:52.082766   31786 crypto.go:164] Writing key to /Users/hardeep/.minikube/profiles/minikube/apiserver.key.dd3b5fb2 ...
I1019 10:23:52.082771   31786 lock.go:35] WriteFile acquiring /Users/hardeep/.minikube/profiles/minikube/apiserver.key.dd3b5fb2: {Name:mk0eb65d7374c887ff0a67fe1b166595076d187f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1019 10:23:52.082980   31786 certs.go:320] copying /Users/hardeep/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2 -> /Users/hardeep/.minikube/profiles/minikube/apiserver.crt
I1019 10:23:52.083491   31786 certs.go:324] copying /Users/hardeep/.minikube/profiles/minikube/apiserver.key.dd3b5fb2 -> /Users/hardeep/.minikube/profiles/minikube/apiserver.key
I1019 10:23:52.083669   31786 certs.go:302] generating aggregator signed cert: /Users/hardeep/.minikube/profiles/minikube/proxy-client.key
I1019 10:23:52.083684   31786 crypto.go:68] Generating cert /Users/hardeep/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I1019 10:23:52.277195   31786 crypto.go:156] Writing cert to /Users/hardeep/.minikube/profiles/minikube/proxy-client.crt ...
I1019 10:23:52.277208   31786 lock.go:35] WriteFile acquiring /Users/hardeep/.minikube/profiles/minikube/proxy-client.crt: {Name:mk76ecfb8c78e0836e8ba5124063878d582a864a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1019 10:23:52.277566   31786 crypto.go:164] Writing key to /Users/hardeep/.minikube/profiles/minikube/proxy-client.key ...
I1019 10:23:52.277572   31786 lock.go:35] WriteFile acquiring /Users/hardeep/.minikube/profiles/minikube/proxy-client.key: {Name:mk04465a0925f40ab590b715c3b570e3eab2a890 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1019 10:23:52.278017   31786 certs.go:388] found cert: /Users/hardeep/.minikube/certs/Users/hardeep/.minikube/certs/ca-key.pem (1679 bytes)
I1019 10:23:52.278058   31786 certs.go:388] found cert: /Users/hardeep/.minikube/certs/Users/hardeep/.minikube/certs/ca.pem (1078 bytes)
I1019 10:23:52.278087   31786 certs.go:388] found cert: /Users/hardeep/.minikube/certs/Users/hardeep/.minikube/certs/cert.pem (1123 bytes)
I1019 10:23:52.278115   31786 certs.go:388] found cert: /Users/hardeep/.minikube/certs/Users/hardeep/.minikube/certs/key.pem (1679 bytes)
I1019 10:23:52.284533   31786 ssh_runner.go:362] scp /Users/hardeep/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I1019 10:23:52.312575   31786 ssh_runner.go:362] scp /Users/hardeep/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I1019 10:23:52.334844   31786 ssh_runner.go:362] scp /Users/hardeep/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1019 10:23:52.362188   31786 ssh_runner.go:362] scp /Users/hardeep/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I1019 10:23:52.391643   31786 ssh_runner.go:362] scp /Users/hardeep/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1019 10:23:52.418820   31786 ssh_runner.go:362] scp /Users/hardeep/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1019 10:23:52.441575   31786 ssh_runner.go:362] scp /Users/hardeep/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1019 10:23:52.469696   31786 ssh_runner.go:362] scp /Users/hardeep/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I1019 10:23:52.495335   31786 ssh_runner.go:362] scp /Users/hardeep/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1019 10:23:52.523921   31786 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1019 10:23:52.541893   31786 ssh_runner.go:195] Run: openssl version
I1019 10:23:52.555141   31786 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1019 10:23:52.574584   31786 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1019 10:23:52.583844   31786 certs.go:431] hashing: -rw-r--r-- 1 root root 1111 Sep 15 07:25 /usr/share/ca-certificates/minikubeCA.pem
I1019 10:23:52.583941   31786 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1019 10:23:52.592038   31786 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1019 10:23:52.603644   31786 kubeadm.go:396] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.25.0 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.25.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath:}
I1019 10:23:52.603747   31786 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1019 10:23:52.644300   31786 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1019 10:23:52.655127   31786 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1019 10:23:52.668846   31786 kubeadm.go:221] ignoring SystemVerification for kubeadm because of docker driver
I1019 10:23:52.668968   31786 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1019 10:23:52.681020   31786 kubeadm.go:152] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I1019 10:23:52.681046   31786 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I1019 10:23:52.758487   31786 kubeadm.go:317] [init] Using Kubernetes version: v1.25.0
I1019 10:23:52.759868   31786 kubeadm.go:317] [preflight] Running pre-flight checks
I1019 10:23:52.915538   31786 kubeadm.go:317] [preflight] Pulling images required for setting up a Kubernetes cluster
I1019 10:23:52.915620   31786 kubeadm.go:317] [preflight] This might take a minute or two, depending on the speed of your internet connection
I1019 10:23:52.915703   31786 kubeadm.go:317] [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
I1019 10:23:53.093566   31786 kubeadm.go:317] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I1019 10:23:53.099341   31786 out.go:204]     ‚ñ™ Generating certificates and keys ...
I1019 10:23:53.099423   31786 kubeadm.go:317] [certs] Using existing ca certificate authority
I1019 10:23:53.099472   31786 kubeadm.go:317] [certs] Using existing apiserver certificate and key on disk
I1019 10:23:53.762041   31786 kubeadm.go:317] [certs] Generating "apiserver-kubelet-client" certificate and key
I1019 10:23:53.970118   31786 kubeadm.go:317] [certs] Generating "front-proxy-ca" certificate and key
I1019 10:23:54.586366   31786 kubeadm.go:317] [certs] Generating "front-proxy-client" certificate and key
I1019 10:23:54.720509   31786 kubeadm.go:317] [certs] Generating "etcd/ca" certificate and key
I1019 10:23:54.934012   31786 kubeadm.go:317] [certs] Generating "etcd/server" certificate and key
I1019 10:23:54.934642   31786 kubeadm.go:317] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I1019 10:23:55.155078   31786 kubeadm.go:317] [certs] Generating "etcd/peer" certificate and key
I1019 10:23:55.155191   31786 kubeadm.go:317] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I1019 10:23:55.268956   31786 kubeadm.go:317] [certs] Generating "etcd/healthcheck-client" certificate and key
I1019 10:23:55.487283   31786 kubeadm.go:317] [certs] Generating "apiserver-etcd-client" certificate and key
I1019 10:23:55.630055   31786 kubeadm.go:317] [certs] Generating "sa" key and public key
I1019 10:23:55.630128   31786 kubeadm.go:317] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I1019 10:23:56.034481   31786 kubeadm.go:317] [kubeconfig] Writing "admin.conf" kubeconfig file
I1019 10:23:56.139613   31786 kubeadm.go:317] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I1019 10:23:56.331312   31786 kubeadm.go:317] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I1019 10:23:56.701103   31786 kubeadm.go:317] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I1019 10:23:56.717902   31786 kubeadm.go:317] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I1019 10:23:56.719126   31786 kubeadm.go:317] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I1019 10:23:56.719587   31786 kubeadm.go:317] [kubelet-start] Starting the kubelet
I1019 10:23:56.829740   31786 kubeadm.go:317] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I1019 10:23:56.836266   31786 out.go:204]     ‚ñ™ Booting up control plane ...
I1019 10:23:56.836352   31786 kubeadm.go:317] [control-plane] Creating static Pod manifest for "kube-apiserver"
I1019 10:23:56.836447   31786 kubeadm.go:317] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I1019 10:23:56.836497   31786 kubeadm.go:317] [control-plane] Creating static Pod manifest for "kube-scheduler"
I1019 10:23:56.836559   31786 kubeadm.go:317] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I1019 10:23:56.837944   31786 kubeadm.go:317] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
I1019 10:24:13.331197   31786 kubeadm.go:317] [apiclient] All control plane components are healthy after 16.505355 seconds
I1019 10:24:13.331393   31786 kubeadm.go:317] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I1019 10:24:13.352388   31786 kubeadm.go:317] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I1019 10:24:13.884729   31786 kubeadm.go:317] [upload-certs] Skipping phase. Please see --upload-certs
I1019 10:24:13.884885   31786 kubeadm.go:317] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I1019 10:24:14.395888   31786 kubeadm.go:317] [bootstrap-token] Using token: 74phd5.g2f9ixe9mv3dkdix
I1019 10:24:14.407156   31786 out.go:204]     ‚ñ™ Configuring RBAC rules ...
I1019 10:24:14.407487   31786 kubeadm.go:317] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I1019 10:24:14.407558   31786 kubeadm.go:317] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I1019 10:24:14.418596   31786 kubeadm.go:317] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I1019 10:24:14.420714   31786 kubeadm.go:317] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I1019 10:24:14.426950   31786 kubeadm.go:317] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I1019 10:24:14.431285   31786 kubeadm.go:317] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I1019 10:24:14.447290   31786 kubeadm.go:317] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I1019 10:24:14.669877   31786 kubeadm.go:317] [addons] Applied essential addon: CoreDNS
I1019 10:24:14.811820   31786 kubeadm.go:317] [addons] Applied essential addon: kube-proxy
I1019 10:24:14.814011   31786 kubeadm.go:317] 
I1019 10:24:14.814065   31786 kubeadm.go:317] Your Kubernetes control-plane has initialized successfully!
I1019 10:24:14.814068   31786 kubeadm.go:317] 
I1019 10:24:14.814126   31786 kubeadm.go:317] To start using your cluster, you need to run the following as a regular user:
I1019 10:24:14.814137   31786 kubeadm.go:317] 
I1019 10:24:14.814186   31786 kubeadm.go:317]   mkdir -p $HOME/.kube
I1019 10:24:14.815570   31786 kubeadm.go:317]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I1019 10:24:14.815609   31786 kubeadm.go:317]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I1019 10:24:14.815611   31786 kubeadm.go:317] 
I1019 10:24:14.815650   31786 kubeadm.go:317] Alternatively, if you are the root user, you can run:
I1019 10:24:14.815653   31786 kubeadm.go:317] 
I1019 10:24:14.815701   31786 kubeadm.go:317]   export KUBECONFIG=/etc/kubernetes/admin.conf
I1019 10:24:14.815705   31786 kubeadm.go:317] 
I1019 10:24:14.815762   31786 kubeadm.go:317] You should now deploy a pod network to the cluster.
I1019 10:24:14.815844   31786 kubeadm.go:317] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I1019 10:24:14.815900   31786 kubeadm.go:317]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I1019 10:24:14.815903   31786 kubeadm.go:317] 
I1019 10:24:14.816504   31786 kubeadm.go:317] You can now join any number of control-plane nodes by copying certificate authorities
I1019 10:24:14.816574   31786 kubeadm.go:317] and service account keys on each node and then running the following as root:
I1019 10:24:14.816577   31786 kubeadm.go:317] 
I1019 10:24:14.817436   31786 kubeadm.go:317]   kubeadm join control-plane.minikube.internal:8443 --token 74phd5.g2f9ixe9mv3dkdix \
I1019 10:24:14.817509   31786 kubeadm.go:317] 	--discovery-token-ca-cert-hash sha256:e2e24b2b591fee088303a39e1b03232d686c83db8159d10ff0128410963984c5 \
I1019 10:24:14.817524   31786 kubeadm.go:317] 	--control-plane 
I1019 10:24:14.817526   31786 kubeadm.go:317] 
I1019 10:24:14.817594   31786 kubeadm.go:317] Then you can join any number of worker nodes by running the following on each as root:
I1019 10:24:14.817604   31786 kubeadm.go:317] 
I1019 10:24:14.818723   31786 kubeadm.go:317] kubeadm join control-plane.minikube.internal:8443 --token 74phd5.g2f9ixe9mv3dkdix \
I1019 10:24:14.818809   31786 kubeadm.go:317] 	--discovery-token-ca-cert-hash sha256:e2e24b2b591fee088303a39e1b03232d686c83db8159d10ff0128410963984c5 
I1019 10:24:14.825817   31786 kubeadm.go:317] W1019 04:53:52.764753     973 initconfiguration.go:119] Usage of CRI endpoints without URL scheme is deprecated and can cause kubelet errors in the future. Automatically prepending scheme "unix" to the "criSocket" with value "/var/run/cri-dockerd.sock". Please update your configuration!
I1019 10:24:14.825954   31786 kubeadm.go:317] 	[WARNING Swap]: swap is enabled; production deployments should disable swap unless testing the NodeSwap feature gate of the kubelet
I1019 10:24:14.826014   31786 kubeadm.go:317] 	[WARNING SystemVerification]: missing optional cgroups: blkio
I1019 10:24:14.826140   31786 kubeadm.go:317] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I1019 10:24:14.826183   31786 cni.go:95] Creating CNI manager for ""
I1019 10:24:14.826198   31786 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I1019 10:24:14.826283   31786 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1019 10:24:14.827018   31786 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.25.0/kubectl label nodes minikube.k8s.io/version=v1.27.0 minikube.k8s.io/commit=4243041b7a72319b9be7842a7d34b6767bbdac2b minikube.k8s.io/name=minikube minikube.k8s.io/updated_at=2022_10_19T10_24_14_0700 minikube.k8s.io/primary=true --all --overwrite --kubeconfig=/var/lib/minikube/kubeconfig
I1019 10:24:14.827032   31786 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.25.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I1019 10:24:14.848225   31786 ops.go:34] apiserver oom_adj: -16
I1019 10:24:15.173206   31786 kubeadm.go:1067] duration metric: took 346.411355ms to wait for elevateKubeSystemPrivileges.
I1019 10:24:15.173329   31786 kubeadm.go:398] StartCluster complete in 22.569746763s
I1019 10:24:15.174034   31786 settings.go:142] acquiring lock: {Name:mkd1f713ebd67acb5b6bd7c1a38b6df503579b5c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1019 10:24:15.174384   31786 settings.go:150] Updating kubeconfig:  /Users/hardeep/.kube/config
I1019 10:24:15.178693   31786 lock.go:35] WriteFile acquiring /Users/hardeep/.kube/config: {Name:mk31b3e81729624a0352cae979e21732d7301be6 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1019 10:24:15.733494   31786 kapi.go:244] deployment "coredns" in namespace "kube-system" and context "minikube" rescaled to 1
I1019 10:24:15.733571   31786 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.25.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1019 10:24:15.733605   31786 start.go:211] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.25.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1019 10:24:15.744217   31786 out.go:177] üîé  Verifying Kubernetes components...
I1019 10:24:15.734867   31786 addons.go:412] enableAddons start: toEnable=map[], additional=[]
I1019 10:24:15.735106   31786 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.25.0
I1019 10:24:15.750205   31786 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I1019 10:24:15.753706   31786 addons.go:65] Setting storage-provisioner=true in profile "minikube"
I1019 10:24:15.753794   31786 addons.go:65] Setting default-storageclass=true in profile "minikube"
I1019 10:24:15.754419   31786 addons.go:153] Setting addon storage-provisioner=true in "minikube"
W1019 10:24:15.754428   31786 addons.go:162] addon storage-provisioner should already be in state true
I1019 10:24:15.755161   31786 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1019 10:24:15.760722   31786 host.go:66] Checking if "minikube" exists ...
I1019 10:24:15.761841   31786 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1019 10:24:15.763416   31786 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1019 10:24:15.988597   31786 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1019 10:24:15.989840   31786 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.25.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.2 host.minikube.internal\n           fallthrough\n        }' | sudo /var/lib/minikube/binaries/v1.25.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I1019 10:24:16.327514   31786 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1019 10:24:16.325606   31786 api_server.go:51] waiting for apiserver process to appear ...
I1019 10:24:16.331869   31786 addons.go:345] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1019 10:24:16.331877   31786 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1019 10:24:16.331953   31786 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1019 10:24:16.332704   31786 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1019 10:24:16.347707   31786 addons.go:153] Setting addon default-storageclass=true in "minikube"
W1019 10:24:16.347727   31786 addons.go:162] addon default-storageclass should already be in state true
I1019 10:24:16.347757   31786 host.go:66] Checking if "minikube" exists ...
I1019 10:24:16.348958   31786 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1019 10:24:16.451809   31786 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54843 SSHKeyPath:/Users/hardeep/.minikube/machines/minikube/id_rsa Username:docker}
I1019 10:24:16.469337   31786 addons.go:345] installing /etc/kubernetes/addons/storageclass.yaml
I1019 10:24:16.469353   31786 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1019 10:24:16.469419   31786 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1019 10:24:16.562998   31786 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54843 SSHKeyPath:/Users/hardeep/.minikube/machines/minikube/id_rsa Username:docker}
I1019 10:24:16.665053   31786 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1019 10:24:16.692037   31786 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1019 10:24:17.677297   31786 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (1.344567304s)
I1019 10:24:17.677309   31786 api_server.go:71] duration metric: took 1.943401207s to wait for apiserver process to appear ...
I1019 10:24:17.677398   31786 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.25.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.2 host.minikube.internal\n           fallthrough\n        }' | sudo /var/lib/minikube/binaries/v1.25.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -": (1.687520935s)
I1019 10:24:17.677412   31786 start.go:810] {"host.minikube.internal": 192.168.65.2} host record injected into CoreDNS
I1019 10:24:17.677608   31786 api_server.go:87] waiting for apiserver healthz status ...
I1019 10:24:17.677623   31786 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:54842/healthz ...
I1019 10:24:17.690741   31786 api_server.go:266] https://127.0.0.1:54842/healthz returned 200:
ok
I1019 10:24:17.695162   31786 api_server.go:140] control plane version: v1.25.0
I1019 10:24:17.695173   31786 api_server.go:130] duration metric: took 17.560683ms to wait for apiserver health ...
I1019 10:24:17.695185   31786 system_pods.go:43] waiting for kube-system pods to appear ...
I1019 10:24:17.708235   31786 system_pods.go:59] 4 kube-system pods found
I1019 10:24:17.708253   31786 system_pods.go:61] "etcd-minikube" [4928d241-b305-4606-8d51-fb9f7cc1a933] Pending
I1019 10:24:17.708261   31786 system_pods.go:61] "kube-apiserver-minikube" [8aaffa3a-ece4-495f-a5a5-822c8836eecf] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1019 10:24:17.708271   31786 system_pods.go:61] "kube-controller-manager-minikube" [64dc2ac9-14d3-411e-8cfc-3b53cddea6b3] Pending
I1019 10:24:17.708276   31786 system_pods.go:61] "kube-scheduler-minikube" [3fd3f8b3-dc58-4bf5-9ab7-48704c4951ef] Running
I1019 10:24:17.708279   31786 system_pods.go:74] duration metric: took 13.091201ms to wait for pod list to return data ...
I1019 10:24:17.708285   31786 kubeadm.go:573] duration metric: took 1.974376518s to wait for : map[apiserver:true system_pods:true] ...
I1019 10:24:17.708296   31786 node_conditions.go:102] verifying NodePressure condition ...
I1019 10:24:17.714887   31786 node_conditions.go:122] node storage ephemeral capacity is 61202244Ki
I1019 10:24:17.714899   31786 node_conditions.go:123] node cpu capacity is 2
I1019 10:24:17.715163   31786 node_conditions.go:105] duration metric: took 6.862443ms to run NodePressure ...
I1019 10:24:17.715176   31786 start.go:216] waiting for startup goroutines ...
I1019 10:24:17.731540   31786 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (1.039455738s)
I1019 10:24:17.731638   31786 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.066554507s)
I1019 10:24:17.787140   31786 out.go:177] üåü  Enabled addons: default-storageclass, storage-provisioner
I1019 10:24:17.791961   31786 addons.go:414] enableAddons completed in 2.057749503s
I1019 10:24:17.888474   31786 start.go:506] kubectl: 1.25.0, cluster: 1.25.0 (minor skew: 0)
I1019 10:24:17.893841   31786 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* -- Logs begin at Wed 2022-10-19 04:53:40 UTC, end at Wed 2022-10-19 05:12:09 UTC. --
Oct 19 04:53:46 minikube dockerd[138]: time="2022-10-19T04:53:46.531568395Z" level=info msg="Processing signal 'terminated'"
Oct 19 04:53:46 minikube dockerd[138]: time="2022-10-19T04:53:46.534007268Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
Oct 19 04:53:46 minikube dockerd[138]: time="2022-10-19T04:53:46.534891595Z" level=info msg="Daemon shutdown complete"
Oct 19 04:53:46 minikube systemd[1]: docker.service: Succeeded.
Oct 19 04:53:46 minikube systemd[1]: Stopped Docker Application Container Engine.
Oct 19 04:53:46 minikube systemd[1]: Starting Docker Application Container Engine...
Oct 19 04:53:46 minikube dockerd[376]: time="2022-10-19T04:53:46.603051468Z" level=info msg="Starting up"
Oct 19 04:53:46 minikube dockerd[376]: time="2022-10-19T04:53:46.605720901Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Oct 19 04:53:46 minikube dockerd[376]: time="2022-10-19T04:53:46.605817222Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Oct 19 04:53:46 minikube dockerd[376]: time="2022-10-19T04:53:46.605876595Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Oct 19 04:53:46 minikube dockerd[376]: time="2022-10-19T04:53:46.605922909Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Oct 19 04:53:46 minikube dockerd[376]: time="2022-10-19T04:53:46.607539957Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Oct 19 04:53:46 minikube dockerd[376]: time="2022-10-19T04:53:46.607634330Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Oct 19 04:53:46 minikube dockerd[376]: time="2022-10-19T04:53:46.607686773Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Oct 19 04:53:46 minikube dockerd[376]: time="2022-10-19T04:53:46.607736699Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Oct 19 04:53:46 minikube dockerd[376]: time="2022-10-19T04:53:46.620501424Z" level=info msg="[graphdriver] using prior storage driver: overlay2"
Oct 19 04:53:46 minikube dockerd[376]: time="2022-10-19T04:53:46.627115969Z" level=info msg="Loading containers: start."
Oct 19 04:53:46 minikube dockerd[376]: time="2022-10-19T04:53:46.743306750Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Oct 19 04:53:46 minikube dockerd[376]: time="2022-10-19T04:53:46.783327615Z" level=info msg="Loading containers: done."
Oct 19 04:53:46 minikube dockerd[376]: time="2022-10-19T04:53:46.794050583Z" level=info msg="Docker daemon" commit=a89b842 graphdriver(s)=overlay2 version=20.10.17
Oct 19 04:53:46 minikube dockerd[376]: time="2022-10-19T04:53:46.794128966Z" level=info msg="Daemon has completed initialization"
Oct 19 04:53:46 minikube systemd[1]: Started Docker Application Container Engine.
Oct 19 04:53:46 minikube dockerd[376]: time="2022-10-19T04:53:46.820404213Z" level=info msg="API listen on [::]:2376"
Oct 19 04:53:46 minikube dockerd[376]: time="2022-10-19T04:53:46.843553317Z" level=info msg="API listen on /var/run/docker.sock"
Oct 19 04:53:49 minikube systemd[1]: Stopping Docker Application Container Engine...
Oct 19 04:53:49 minikube dockerd[376]: time="2022-10-19T04:53:49.145590672Z" level=info msg="Processing signal 'terminated'"
Oct 19 04:53:49 minikube dockerd[376]: time="2022-10-19T04:53:49.147305409Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
Oct 19 04:53:49 minikube dockerd[376]: time="2022-10-19T04:53:49.148161472Z" level=info msg="Daemon shutdown complete"
Oct 19 04:53:49 minikube systemd[1]: docker.service: Succeeded.
Oct 19 04:53:49 minikube systemd[1]: Stopped Docker Application Container Engine.
Oct 19 04:53:49 minikube systemd[1]: Starting Docker Application Container Engine...
Oct 19 04:53:49 minikube dockerd[582]: time="2022-10-19T04:53:49.222917703Z" level=info msg="Starting up"
Oct 19 04:53:49 minikube dockerd[582]: time="2022-10-19T04:53:49.225764584Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Oct 19 04:53:49 minikube dockerd[582]: time="2022-10-19T04:53:49.225871128Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Oct 19 04:53:49 minikube dockerd[582]: time="2022-10-19T04:53:49.225949557Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Oct 19 04:53:49 minikube dockerd[582]: time="2022-10-19T04:53:49.226012798Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Oct 19 04:53:49 minikube dockerd[582]: time="2022-10-19T04:53:49.228135270Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Oct 19 04:53:49 minikube dockerd[582]: time="2022-10-19T04:53:49.228537121Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Oct 19 04:53:49 minikube dockerd[582]: time="2022-10-19T04:53:49.228664112Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Oct 19 04:53:49 minikube dockerd[582]: time="2022-10-19T04:53:49.228773323Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Oct 19 04:53:49 minikube dockerd[582]: time="2022-10-19T04:53:49.235732656Z" level=info msg="[graphdriver] using prior storage driver: overlay2"
Oct 19 04:53:49 minikube dockerd[582]: time="2022-10-19T04:53:49.245684872Z" level=info msg="Loading containers: start."
Oct 19 04:53:49 minikube dockerd[582]: time="2022-10-19T04:53:49.413561213Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Oct 19 04:53:49 minikube dockerd[582]: time="2022-10-19T04:53:49.567668918Z" level=info msg="Loading containers: done."
Oct 19 04:53:49 minikube dockerd[582]: time="2022-10-19T04:53:49.584876965Z" level=info msg="Docker daemon" commit=a89b842 graphdriver(s)=overlay2 version=20.10.17
Oct 19 04:53:49 minikube dockerd[582]: time="2022-10-19T04:53:49.585006665Z" level=info msg="Daemon has completed initialization"
Oct 19 04:53:49 minikube systemd[1]: Started Docker Application Container Engine.
Oct 19 04:53:49 minikube dockerd[582]: time="2022-10-19T04:53:49.620944748Z" level=info msg="API listen on [::]:2376"
Oct 19 04:53:49 minikube dockerd[582]: time="2022-10-19T04:53:49.639014283Z" level=info msg="API listen on /var/run/docker.sock"
Oct 19 04:54:29 minikube dockerd[582]: time="2022-10-19T04:54:29.264918345Z" level=warning msg="reference for unknown type: " digest="sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660" remote="k8s.gcr.io/ingress-nginx/kube-webhook-certgen@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660"
Oct 19 04:54:37 minikube dockerd[582]: time="2022-10-19T04:54:37.709117571Z" level=info msg="ignoring event" container=5121e22ca47e92f985f55037be3c22deddb95459ed1a6a5cdf87d62ed4cce054 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 19 04:54:37 minikube dockerd[582]: time="2022-10-19T04:54:37.822666917Z" level=info msg="ignoring event" container=1ffa2792a43f7c3c1e01198818c4d5d67588e7c97a8e290eaac7a098bc1a7ecb module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 19 04:54:38 minikube dockerd[582]: time="2022-10-19T04:54:38.868889655Z" level=info msg="ignoring event" container=30da5fa0b7b445d45d822f7bfd2c79ee0fb1e98ea191b66ca26b7f2cf45ce4dd module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 19 04:54:39 minikube dockerd[582]: time="2022-10-19T04:54:39.897999476Z" level=info msg="ignoring event" container=aecaae456ae8384ea3a45cb938662350b177aa737727db9bcc252dfa39505f7d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 19 04:54:40 minikube dockerd[582]: time="2022-10-19T04:54:40.910075415Z" level=info msg="ignoring event" container=6e9427276b29dc7b98e87c68acd7ef8b11f6cc01ab655dc6be2d001ca1a55e69 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 19 04:55:02 minikube dockerd[582]: time="2022-10-19T04:55:02.197126850Z" level=info msg="ignoring event" container=69c68b0651b0f5c936de78a4fb6901d18d307d63820409c39806c50eee86e0df module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 19 04:57:17 minikube dockerd[582]: time="2022-10-19T04:57:17.622524367Z" level=warning msg="reference for unknown type: " digest="sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8" remote="k8s.gcr.io/ingress-nginx/controller@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8"
Oct 19 05:10:45 minikube dockerd[582]: time="2022-10-19T05:10:45.888082618Z" level=error msg="stream copy error: reading from a closed fifo"
Oct 19 05:10:45 minikube dockerd[582]: time="2022-10-19T05:10:45.889126536Z" level=error msg="stream copy error: reading from a closed fifo"
Oct 19 05:10:45 minikube dockerd[582]: time="2022-10-19T05:10:45.902073562Z" level=error msg="Error running exec 0df45f03faa09f57b0f81b7a4f9ffa31d015c1275d11cd95eca7215604a1fa54 in container: OCI runtime exec failed: exec failed: unable to start container process: exec: \"bin/bash\": stat bin/bash: no such file or directory: unknown"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                                                   CREATED             STATE               NAME                      ATTEMPT             POD ID
b75504f43a846       k8s.gcr.io/ingress-nginx/controller@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8             14 minutes ago      Running             controller                0                   c4ed66d1780a4
3f88c8f4e3d18       abhinavdobhal/pwa-flask@sha256:69027f5ca3680b81b4d20944a4bcb42a3e8f113deb949f4b5e7a358f8644437d                         14 minutes ago      Running             flask                     0                   12f63d99e4d72
eb4b7469a1acd       hardeep4770/next-nginx-app@sha256:45f449b19cec1ff493b951ffb1863159bacaa2d540c6583617d9bee23df88de4                      14 minutes ago      Running             next-app                  0                   b2f22dfb07d40
f12589aa66e9f       hardeep4770/next-nginx-app@sha256:45f449b19cec1ff493b951ffb1863159bacaa2d540c6583617d9bee23df88de4                      15 minutes ago      Running             next-app                  0                   966e445317e2d
8068a81f39882       hardeep4770/next-nginx-app@sha256:45f449b19cec1ff493b951ffb1863159bacaa2d540c6583617d9bee23df88de4                      15 minutes ago      Running             next-app                  0                   46a9774750f99
42fa73496d8f7       abhinavdobhal/pwa-flask@sha256:69027f5ca3680b81b4d20944a4bcb42a3e8f113deb949f4b5e7a358f8644437d                         15 minutes ago      Running             flask                     0                   88c16f539e3d0
5ea4e4f63397b       abhinavdobhal/pwa-flask@sha256:69027f5ca3680b81b4d20944a4bcb42a3e8f113deb949f4b5e7a358f8644437d                         15 minutes ago      Running             flask                     0                   822c41e30f9cd
670ca3b75e22c       6e38f40d628db                                                                                                           17 minutes ago      Running             storage-provisioner       1                   d2be220e7886a
30da5fa0b7b44       c41e9fcadf5a2                                                                                                           17 minutes ago      Exited              patch                     1                   6e9427276b29d
1ffa2792a43f7       k8s.gcr.io/ingress-nginx/kube-webhook-certgen@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660   17 minutes ago      Exited              create                    0                   aecaae456ae83
d20c45244a526       5185b96f0becf                                                                                                           17 minutes ago      Running             coredns                   0                   36743ce6b7b8e
32e99e79a509a       58a9a0c6d96f2                                                                                                           17 minutes ago      Running             kube-proxy                0                   baced79cf32be
69c68b0651b0f       6e38f40d628db                                                                                                           17 minutes ago      Exited              storage-provisioner       0                   d2be220e7886a
fab5befbe11e2       bef2cf3115095                                                                                                           18 minutes ago      Running             kube-scheduler            0                   3a7bd339bee2c
deca9f920feec       1a54c86c03a67                                                                                                           18 minutes ago      Running             kube-controller-manager   0                   2784312020c56
7f2a96ff21328       4d2edfd10d3e3                                                                                                           18 minutes ago      Running             kube-apiserver            0                   1700094b747c1
e94ddff3af5f2       a8a176a5d5d69                                                                                                           18 minutes ago      Running             etcd                      0                   159c282e9358d

* 
* ==> coredns [d20c45244a52] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = a1b5920ef1e8e10875eeec3214b810e7e404fdaf6cfe53f31cc42ae1e9ba5884ecf886330489b6b02fba5b37a31406fcb402b2501c7ab0318fc890d74b6fae55
CoreDNS-1.9.3
linux/amd64, go1.18.2, 45b0a11
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=4243041b7a72319b9be7842a7d34b6767bbdac2b
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2022_10_19T10_24_14_0700
                    minikube.k8s.io/version=v1.27.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 19 Oct 2022 04:54:11 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Wed, 19 Oct 2022 05:12:08 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Wed, 19 Oct 2022 05:08:31 +0000   Wed, 19 Oct 2022 04:54:10 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 19 Oct 2022 05:08:31 +0000   Wed, 19 Oct 2022 04:54:10 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 19 Oct 2022 05:08:31 +0000   Wed, 19 Oct 2022 04:54:10 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 19 Oct 2022 05:08:31 +0000   Wed, 19 Oct 2022 04:54:25 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                2
  ephemeral-storage:  61202244Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             4029200Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  61202244Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             4029200Ki
  pods:               110
System Info:
  Machine ID:                 40026c506a9948afaae3b0fda0a61c83
  System UUID:                4170a4ae-d113-4ddd-a989-4e314351960f
  Boot ID:                    e7d33327-7844-40f1-a84c-8fda34910053
  Kernel Version:             5.10.124-linuxkit
  OS Image:                   Ubuntu 20.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://20.10.17
  Kubelet Version:            v1.25.0
  Kube-Proxy Version:         v1.25.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (14 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  default                     flask-deployment-d5d4d9df6-2v5pf             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         17m
  default                     flask-deployment-d5d4d9df6-6nwwc             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         17m
  default                     flask-deployment-d5d4d9df6-rrgrj             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         17m
  default                     frontend-deployment-848857f9c8-n2jfb         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         17m
  default                     frontend-deployment-848857f9c8-nc4qp         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         17m
  default                     frontend-deployment-848857f9c8-xtxfs         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         17m
  ingress-nginx               ingress-nginx-controller-5959f988fd-45kbr    100m (5%!)(MISSING)     0 (0%!)(MISSING)      90Mi (2%!)(MISSING)        0 (0%!)(MISSING)         17m
  kube-system                 coredns-565d847f94-rvtjd                     100m (5%!)(MISSING)     0 (0%!)(MISSING)      70Mi (1%!)(MISSING)        170Mi (4%!)(MISSING)     17m
  kube-system                 etcd-minikube                                100m (5%!)(MISSING)     0 (0%!)(MISSING)      100Mi (2%!)(MISSING)       0 (0%!)(MISSING)         17m
  kube-system                 kube-apiserver-minikube                      250m (12%!)(MISSING)    0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         17m
  kube-system                 kube-controller-manager-minikube             200m (10%!)(MISSING)    0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         17m
  kube-system                 kube-proxy-42s8s                             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         17m
  kube-system                 kube-scheduler-minikube                      100m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         17m
  kube-system                 storage-provisioner                          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         17m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (42%!)(MISSING)  0 (0%!)(MISSING)
  memory             260Mi (6%!)(MISSING)  170Mi (4%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 17m                kube-proxy       
  Normal  NodeHasSufficientMemory  18m (x5 over 18m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    18m (x5 over 18m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     18m (x4 over 18m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  Starting                 17m                kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  17m                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    17m                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     17m                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  17m                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeReady                17m                kubelet          Node minikube status is now: NodeReady
  Normal  RegisteredNode           17m                node-controller  Node minikube event: Registered Node minikube in Controller

* 
* ==> dmesg <==
* [Oct19 04:37] ERROR: earlyprintk= earlyser already used
[  +0.000000] ERROR: earlyprintk= earlyser already used
[  +0.000000] ACPI BIOS Warning (bug): Incorrect checksum in table [DSDT] - 0x7E, should be 0xDB (20200925/tbprint-173)
[  +0.003799] ACPI: setting ELCR to 0200 (from 06e0)
[  +3.403896] Hangcheck: starting hangcheck timer 0.9.1 (tick is 180 seconds, margin is 60 seconds).
[  +0.009464] the cryptoloop driver has been deprecated and will be removed in in Linux 5.16
[  +0.029989] ACPI Error: Could not enable RealTimeClock event (20200925/evxfevnt-182)
[  +0.002068] ACPI Warning: Could not enable fixed event - RealTimeClock (4) (20200925/evxface-618)
[  +4.502450] grpcfuse: loading out-of-tree module taints kernel.
[  +0.158794] hrtimer: interrupt took 9432081 ns

* 
* ==> etcd [e94ddff3af5f] <==
* {"level":"info","ts":"2022-10-19T04:54:06.192Z","caller":"embed/etcd.go:581","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2022-10-19T04:54:06.192Z","caller":"embed/etcd.go:553","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2022-10-19T04:54:06.195Z","caller":"embed/etcd.go:277","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2022-10-19T04:54:06.195Z","caller":"embed/etcd.go:763","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2022-10-19T04:54:06.759Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 1"}
{"level":"info","ts":"2022-10-19T04:54:06.759Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 1"}
{"level":"info","ts":"2022-10-19T04:54:06.759Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 1"}
{"level":"info","ts":"2022-10-19T04:54:06.760Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 2"}
{"level":"info","ts":"2022-10-19T04:54:06.760Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2022-10-19T04:54:06.760Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 2"}
{"level":"info","ts":"2022-10-19T04:54:06.760Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 2"}
{"level":"info","ts":"2022-10-19T04:54:06.761Z","caller":"etcdserver/server.go:2042","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2022-10-19T04:54:06.761Z","caller":"embed/serve.go:98","msg":"ready to serve client requests"}
{"level":"info","ts":"2022-10-19T04:54:06.766Z","caller":"embed/serve.go:188","msg":"serving client traffic securely","address":"127.0.0.1:2379"}
{"level":"info","ts":"2022-10-19T04:54:06.770Z","caller":"embed/serve.go:98","msg":"ready to serve client requests"}
{"level":"info","ts":"2022-10-19T04:54:06.773Z","caller":"embed/serve.go:188","msg":"serving client traffic securely","address":"192.168.49.2:2379"}
{"level":"info","ts":"2022-10-19T04:54:06.775Z","caller":"etcdserver/server.go:2507","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2022-10-19T04:54:06.796Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2022-10-19T04:54:06.796Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2022-10-19T04:54:06.796Z","caller":"etcdserver/server.go:2531","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2022-10-19T04:54:06.801Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2022-10-19T04:54:06.801Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2022-10-19T04:55:28.900Z","caller":"traceutil/trace.go:171","msg":"trace[825867089] linearizableReadLoop","detail":"{readStateIndex:590; appliedIndex:590; }","duration":"167.893171ms","start":"2022-10-19T04:55:28.732Z","end":"2022-10-19T04:55:28.900Z","steps":["trace[825867089] 'read index received'  (duration: 167.88423ms)","trace[825867089] 'applied index is now lower than readState.Index'  (duration: 8.198¬µs)"],"step_count":2}
{"level":"warn","ts":"2022-10-19T04:55:28.902Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"169.273236ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2022-10-19T04:55:28.902Z","caller":"traceutil/trace.go:171","msg":"trace[1773856416] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:563; }","duration":"169.522556ms","start":"2022-10-19T04:55:28.732Z","end":"2022-10-19T04:55:28.902Z","steps":["trace[1773856416] 'agreement among raft nodes before linearized reading'  (duration: 168.216193ms)"],"step_count":1}
{"level":"warn","ts":"2022-10-19T04:55:28.903Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"169.830347ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/ingress-nginx/\" range_end:\"/registry/pods/ingress-nginx0\" ","response":"range_response_count:3 size:13329"}
{"level":"info","ts":"2022-10-19T04:55:28.903Z","caller":"traceutil/trace.go:171","msg":"trace[2051739298] range","detail":"{range_begin:/registry/pods/ingress-nginx/; range_end:/registry/pods/ingress-nginx0; response_count:3; response_revision:563; }","duration":"170.298155ms","start":"2022-10-19T04:55:28.733Z","end":"2022-10-19T04:55:28.903Z","steps":["trace[2051739298] 'agreement among raft nodes before linearized reading'  (duration: 167.995824ms)"],"step_count":1}
{"level":"info","ts":"2022-10-19T04:55:38.545Z","caller":"traceutil/trace.go:171","msg":"trace[769072347] linearizableReadLoop","detail":"{readStateIndex:598; appliedIndex:598; }","duration":"146.003953ms","start":"2022-10-19T04:55:38.398Z","end":"2022-10-19T04:55:38.544Z","steps":["trace[769072347] 'read index received'  (duration: 145.994008ms)","trace[769072347] 'applied index is now lower than readState.Index'  (duration: 9.047¬µs)"],"step_count":2}
{"level":"warn","ts":"2022-10-19T04:55:38.574Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"175.211122ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/apiregistration.k8s.io/apiservices/\" range_end:\"/registry/apiregistration.k8s.io/apiservices0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2022-10-19T04:55:38.574Z","caller":"traceutil/trace.go:171","msg":"trace[979761818] range","detail":"{range_begin:/registry/apiregistration.k8s.io/apiservices/; range_end:/registry/apiregistration.k8s.io/apiservices0; response_count:0; response_revision:570; }","duration":"175.36438ms","start":"2022-10-19T04:55:38.398Z","end":"2022-10-19T04:55:38.574Z","steps":["trace[979761818] 'agreement among raft nodes before linearized reading'  (duration: 146.268238ms)","trace[979761818] 'count revisions from in-memory index tree'  (duration: 28.92607ms)"],"step_count":2}
{"level":"warn","ts":"2022-10-19T04:55:38.575Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"128.260378ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2022-10-19T04:55:38.575Z","caller":"traceutil/trace.go:171","msg":"trace[1689280844] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:570; }","duration":"128.305403ms","start":"2022-10-19T04:55:38.446Z","end":"2022-10-19T04:55:38.575Z","steps":["trace[1689280844] 'agreement among raft nodes before linearized reading'  (duration: 98.576751ms)","trace[1689280844] 'range keys from in-memory index tree'  (duration: 29.673104ms)"],"step_count":2}
{"level":"info","ts":"2022-10-19T04:55:50.557Z","caller":"traceutil/trace.go:171","msg":"trace[404411935] linearizableReadLoop","detail":"{readStateIndex:609; appliedIndex:609; }","duration":"110.748635ms","start":"2022-10-19T04:55:50.446Z","end":"2022-10-19T04:55:50.557Z","steps":["trace[404411935] 'read index received'  (duration: 110.743957ms)","trace[404411935] 'applied index is now lower than readState.Index'  (duration: 3.927¬µs)"],"step_count":2}
{"level":"warn","ts":"2022-10-19T04:55:50.559Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"112.373327ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2022-10-19T04:55:50.559Z","caller":"traceutil/trace.go:171","msg":"trace[1102339201] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:578; }","duration":"113.087069ms","start":"2022-10-19T04:55:50.446Z","end":"2022-10-19T04:55:50.559Z","steps":["trace[1102339201] 'agreement among raft nodes before linearized reading'  (duration: 110.890677ms)"],"step_count":1}
{"level":"info","ts":"2022-10-19T04:56:21.044Z","caller":"traceutil/trace.go:171","msg":"trace[2101189015] linearizableReadLoop","detail":"{readStateIndex:636; appliedIndex:636; }","duration":"200.126785ms","start":"2022-10-19T04:56:20.843Z","end":"2022-10-19T04:56:21.044Z","steps":["trace[2101189015] 'read index received'  (duration: 200.12191ms)","trace[2101189015] 'applied index is now lower than readState.Index'  (duration: 4.187¬µs)"],"step_count":2}
{"level":"warn","ts":"2022-10-19T04:56:21.090Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"246.604469ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterroles/\" range_end:\"/registry/clusterroles0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2022-10-19T04:56:21.090Z","caller":"traceutil/trace.go:171","msg":"trace[1640063847] range","detail":"{range_begin:/registry/clusterroles/; range_end:/registry/clusterroles0; response_count:0; response_revision:599; }","duration":"246.683546ms","start":"2022-10-19T04:56:20.843Z","end":"2022-10-19T04:56:21.090Z","steps":["trace[1640063847] 'agreement among raft nodes before linearized reading'  (duration: 200.256799ms)","trace[1640063847] 'count revisions from in-memory index tree'  (duration: 46.336764ms)"],"step_count":2}
{"level":"warn","ts":"2022-10-19T04:56:21.090Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"188.867858ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/mutatingwebhookconfigurations/\" range_end:\"/registry/mutatingwebhookconfigurations0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2022-10-19T04:56:21.090Z","caller":"traceutil/trace.go:171","msg":"trace[55849072] range","detail":"{range_begin:/registry/mutatingwebhookconfigurations/; range_end:/registry/mutatingwebhookconfigurations0; response_count:0; response_revision:599; }","duration":"188.930317ms","start":"2022-10-19T04:56:20.901Z","end":"2022-10-19T04:56:21.090Z","steps":["trace[55849072] 'agreement among raft nodes before linearized reading'  (duration: 142.318172ms)","trace[55849072] 'count revisions from in-memory index tree'  (duration: 46.543199ms)"],"step_count":2}
{"level":"info","ts":"2022-10-19T04:56:23.614Z","caller":"traceutil/trace.go:171","msg":"trace[1653793882] linearizableReadLoop","detail":"{readStateIndex:639; appliedIndex:638; }","duration":"190.510432ms","start":"2022-10-19T04:56:23.423Z","end":"2022-10-19T04:56:23.614Z","steps":["trace[1653793882] 'read index received'  (duration: 189.443703ms)","trace[1653793882] 'applied index is now lower than readState.Index'  (duration: 1.066264ms)"],"step_count":2}
{"level":"info","ts":"2022-10-19T04:56:23.614Z","caller":"traceutil/trace.go:171","msg":"trace[1558327943] transaction","detail":"{read_only:false; response_revision:601; number_of_response:1; }","duration":"383.954486ms","start":"2022-10-19T04:56:23.230Z","end":"2022-10-19T04:56:23.614Z","steps":["trace[1558327943] 'process raft request'  (duration: 382.956214ms)"],"step_count":1}
{"level":"warn","ts":"2022-10-19T04:56:23.614Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"190.829587ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2022-10-19T04:56:23.614Z","caller":"traceutil/trace.go:171","msg":"trace[1313293876] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:601; }","duration":"190.858077ms","start":"2022-10-19T04:56:23.423Z","end":"2022-10-19T04:56:23.614Z","steps":["trace[1313293876] 'agreement among raft nodes before linearized reading'  (duration: 190.820905ms)"],"step_count":1}
{"level":"warn","ts":"2022-10-19T04:56:23.614Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-10-19T04:56:23.230Z","time spent":"383.993025ms","remote":"127.0.0.1:33276","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":116,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:594 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:65 lease:8128016488263210428 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >"}
{"level":"warn","ts":"2022-10-19T04:56:28.202Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"120.665514ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/ingress-nginx/\" range_end:\"/registry/pods/ingress-nginx0\" ","response":"range_response_count:3 size:13329"}
{"level":"info","ts":"2022-10-19T04:56:28.202Z","caller":"traceutil/trace.go:171","msg":"trace[860811845] range","detail":"{range_begin:/registry/pods/ingress-nginx/; range_end:/registry/pods/ingress-nginx0; response_count:3; response_revision:605; }","duration":"120.755747ms","start":"2022-10-19T04:56:28.082Z","end":"2022-10-19T04:56:28.202Z","steps":["trace[860811845] 'range keys from in-memory index tree'  (duration: 120.52277ms)"],"step_count":1}
{"level":"info","ts":"2022-10-19T04:57:53.307Z","caller":"traceutil/trace.go:171","msg":"trace[494441959] linearizableReadLoop","detail":"{readStateIndex:769; appliedIndex:769; }","duration":"141.964846ms","start":"2022-10-19T04:57:53.165Z","end":"2022-10-19T04:57:53.307Z","steps":["trace[494441959] 'read index received'  (duration: 141.958467ms)","trace[494441959] 'applied index is now lower than readState.Index'  (duration: 5.423¬µs)"],"step_count":2}
{"level":"warn","ts":"2022-10-19T04:57:53.309Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"143.810913ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/namespaces/default\" ","response":"range_response_count:1 size:341"}
{"level":"info","ts":"2022-10-19T04:57:53.309Z","caller":"traceutil/trace.go:171","msg":"trace[93542286] range","detail":"{range_begin:/registry/namespaces/default; range_end:; response_count:1; response_revision:713; }","duration":"143.929319ms","start":"2022-10-19T04:57:53.165Z","end":"2022-10-19T04:57:53.309Z","steps":["trace[93542286] 'agreement among raft nodes before linearized reading'  (duration: 142.205952ms)"],"step_count":1}
{"level":"warn","ts":"2022-10-19T04:57:57.866Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"184.479347ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/runtimeclasses/\" range_end:\"/registry/runtimeclasses0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2022-10-19T04:57:57.867Z","caller":"traceutil/trace.go:171","msg":"trace[935320762] range","detail":"{range_begin:/registry/runtimeclasses/; range_end:/registry/runtimeclasses0; response_count:0; response_revision:717; }","duration":"185.076773ms","start":"2022-10-19T04:57:57.681Z","end":"2022-10-19T04:57:57.867Z","steps":["trace[935320762] 'count revisions from in-memory index tree'  (duration: 184.313389ms)"],"step_count":1}
{"level":"warn","ts":"2022-10-19T04:57:57.866Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"278.208975ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/ingress-nginx/\" range_end:\"/registry/pods/ingress-nginx0\" ","response":"range_response_count:3 size:13329"}
{"level":"info","ts":"2022-10-19T04:57:57.867Z","caller":"traceutil/trace.go:171","msg":"trace[22338697] range","detail":"{range_begin:/registry/pods/ingress-nginx/; range_end:/registry/pods/ingress-nginx0; response_count:3; response_revision:717; }","duration":"279.194637ms","start":"2022-10-19T04:57:57.588Z","end":"2022-10-19T04:57:57.867Z","steps":["trace[22338697] 'range keys from in-memory index tree'  (duration: 278.07364ms)"],"step_count":1}
{"level":"info","ts":"2022-10-19T04:57:59.332Z","caller":"traceutil/trace.go:171","msg":"trace[1431542571] transaction","detail":"{read_only:false; response_revision:728; number_of_response:1; }","duration":"169.532499ms","start":"2022-10-19T04:57:59.163Z","end":"2022-10-19T04:57:59.332Z","steps":["trace[1431542571] 'process raft request'  (duration: 168.953704ms)"],"step_count":1}
{"level":"info","ts":"2022-10-19T05:00:53.201Z","caller":"traceutil/trace.go:171","msg":"trace[1204771513] transaction","detail":"{read_only:false; response_revision:889; number_of_response:1; }","duration":"144.530901ms","start":"2022-10-19T05:00:53.056Z","end":"2022-10-19T05:00:53.201Z","steps":["trace[1204771513] 'process raft request'  (duration: 143.637631ms)"],"step_count":1}
{"level":"info","ts":"2022-10-19T05:04:07.696Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":801}
{"level":"info","ts":"2022-10-19T05:04:07.708Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":801,"took":"11.865883ms"}
{"level":"info","ts":"2022-10-19T05:09:07.480Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1052}
{"level":"info","ts":"2022-10-19T05:09:07.482Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":1052,"took":"1.018746ms"}

* 
* ==> kernel <==
*  05:12:10 up 35 min,  0 users,  load average: 0.91, 1.04, 0.89
Linux minikube 5.10.124-linuxkit #1 SMP Thu Jun 30 08:19:10 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 20.04.5 LTS"

* 
* ==> kube-apiserver [7f2a96ff2132] <==
* I1019 04:54:11.479703       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1019 04:54:11.480121       1 controller.go:80] Starting OpenAPI V3 AggregationController
I1019 04:54:11.480375       1 available_controller.go:491] Starting AvailableConditionController
I1019 04:54:11.480438       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I1019 04:54:11.480547       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I1019 04:54:11.481402       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I1019 04:54:11.481481       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I1019 04:54:11.481542       1 controller.go:83] Starting OpenAPI AggregationController
I1019 04:54:11.481788       1 apf_controller.go:300] Starting API Priority and Fairness config controller
I1019 04:54:11.486110       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I1019 04:54:11.486150       1 shared_informer.go:255] Waiting for caches to sync for cluster_authentication_trust_controller
I1019 04:54:11.486419       1 customresource_discovery_controller.go:209] Starting DiscoveryController
I1019 04:54:11.493989       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I1019 04:54:11.494173       1 shared_informer.go:255] Waiting for caches to sync for crd-autoregister
I1019 04:54:11.514942       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1019 04:54:11.515653       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1019 04:54:11.519335       1 controller.go:85] Starting OpenAPI controller
I1019 04:54:11.519443       1 controller.go:85] Starting OpenAPI V3 controller
I1019 04:54:11.519510       1 naming_controller.go:291] Starting NamingConditionController
I1019 04:54:11.519613       1 establishing_controller.go:76] Starting EstablishingController
I1019 04:54:11.519654       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I1019 04:54:11.519666       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1019 04:54:11.519752       1 crd_finalizer.go:266] Starting CRDFinalizer
I1019 04:54:11.599919       1 shared_informer.go:262] Caches are synced for crd-autoregister
I1019 04:54:11.619287       1 controller.go:616] quota admission added evaluator for: namespaces
I1019 04:54:11.651765       1 shared_informer.go:262] Caches are synced for node_authorizer
I1019 04:54:11.680400       1 cache.go:39] Caches are synced for autoregister controller
I1019 04:54:11.680889       1 cache.go:39] Caches are synced for AvailableConditionController controller
I1019 04:54:11.681561       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1019 04:54:11.681997       1 apf_controller.go:305] Running API Priority and Fairness config worker
I1019 04:54:11.686693       1 shared_informer.go:262] Caches are synced for cluster_authentication_trust_controller
I1019 04:54:12.207925       1 controller.go:132] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).
I1019 04:54:12.489324       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I1019 04:54:12.494308       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I1019 04:54:12.494361       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1019 04:54:13.004991       1 controller.go:616] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I1019 04:54:13.071074       1 controller.go:616] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I1019 04:54:13.136874       1 alloc.go:327] "allocated clusterIPs" service="default/kubernetes" clusterIPs=map[IPv4:10.96.0.1]
W1019 04:54:13.144514       1 lease.go:250] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I1019 04:54:13.145658       1 controller.go:616] quota admission added evaluator for: endpoints
I1019 04:54:13.149759       1 controller.go:616] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1019 04:54:13.600740       1 controller.go:616] quota admission added evaluator for: serviceaccounts
I1019 04:54:14.664590       1 controller.go:616] quota admission added evaluator for: deployments.apps
I1019 04:54:14.680481       1 alloc.go:327] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs=map[IPv4:10.96.0.10]
I1019 04:54:14.693241       1 controller.go:616] quota admission added evaluator for: daemonsets.apps
I1019 04:54:14.804890       1 controller.go:616] quota admission added evaluator for: leases.coordination.k8s.io
I1019 04:54:23.610039       1 alloc.go:327] "allocated clusterIPs" service="default/flask-service" clusterIPs=map[IPv4:10.110.225.237]
I1019 04:54:23.988425       1 alloc.go:327] "allocated clusterIPs" service="default/frontend-service" clusterIPs=map[IPv4:10.99.19.3]
I1019 04:54:24.942996       1 alloc.go:327] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller" clusterIPs=map[IPv4:10.99.93.209]
I1019 04:54:24.960372       1 alloc.go:327] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller-admission" clusterIPs=map[IPv4:10.108.53.14]
I1019 04:54:24.985749       1 controller.go:616] quota admission added evaluator for: jobs.batch
I1019 04:54:28.069304       1 controller.go:616] quota admission added evaluator for: replicasets.apps
I1019 04:54:28.220918       1 controller.go:616] quota admission added evaluator for: controllerrevisions.apps
I1019 04:58:00.151138       1 controller.go:616] quota admission added evaluator for: ingresses.networking.k8s.io
I1019 05:01:13.988606       1 trace.go:205] Trace[667250076]: "Get" url:/api/v1/namespaces/default,user-agent:kube-apiserver/v1.25.0 (linux/amd64) kubernetes/a866cbe,audit-id:141142ea-10a6-4835-ae82-77fc2e37e455,client:127.0.0.1,accept:application/vnd.kubernetes.protobuf, */*,protocol:HTTP/2.0 (19-Oct-2022 05:01:13.031) (total time: 957ms):
Trace[667250076]: ---"About to write a response" 957ms (05:01:13.988)
Trace[667250076]: [957.053487ms] [957.053487ms] END
I1019 05:01:13.992202       1 trace.go:205] Trace[868774510]: "Get" url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:ccf1df4a-510a-4d0e-b302-c895080b96d8,client:192.168.49.2,accept:application/json, */*,protocol:HTTP/2.0 (19-Oct-2022 05:01:12.663) (total time: 1328ms):
Trace[868774510]: ---"About to write a response" 1328ms (05:01:13.992)
Trace[868774510]: [1.328840363s] [1.328840363s] END

* 
* ==> kube-controller-manager [deca9f920fee] <==
* I1019 04:54:27.434427       1 shared_informer.go:262] Caches are synced for attach detach
I1019 04:54:27.440296       1 shared_informer.go:262] Caches are synced for daemon sets
I1019 04:54:27.460508       1 shared_informer.go:262] Caches are synced for GC
I1019 04:54:27.461910       1 shared_informer.go:262] Caches are synced for TTL
I1019 04:54:27.463281       1 shared_informer.go:262] Caches are synced for endpoint_slice
I1019 04:54:27.463785       1 shared_informer.go:262] Caches are synced for taint
I1019 04:54:27.463863       1 node_lifecycle_controller.go:1443] Initializing eviction metric for zone: 
W1019 04:54:27.463914       1 node_lifecycle_controller.go:1058] Missing timestamp for Node minikube. Assuming now as a timestamp.
I1019 04:54:27.463965       1 node_lifecycle_controller.go:1259] Controller detected that zone  is now in state Normal.
I1019 04:54:27.464233       1 taint_manager.go:204] "Starting NoExecuteTaintManager"
I1019 04:54:27.464349       1 taint_manager.go:209] "Sending events to api server"
I1019 04:54:27.464436       1 event.go:294] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I1019 04:54:27.477630       1 shared_informer.go:262] Caches are synced for node
I1019 04:54:27.477670       1 range_allocator.go:166] Starting range CIDR allocator
I1019 04:54:27.477676       1 shared_informer.go:255] Waiting for caches to sync for cidrallocator
I1019 04:54:27.477684       1 shared_informer.go:262] Caches are synced for cidrallocator
I1019 04:54:27.500672       1 range_allocator.go:367] Set node minikube PodCIDR to [10.244.0.0/24]
I1019 04:54:27.519189       1 shared_informer.go:262] Caches are synced for persistent volume
I1019 04:54:27.856404       1 shared_informer.go:262] Caches are synced for garbage collector
I1019 04:54:27.913595       1 shared_informer.go:262] Caches are synced for garbage collector
I1019 04:54:27.913631       1 garbagecollector.go:163] Garbage collector: all resource monitors have synced. Proceeding to collect garbage
I1019 04:54:28.076129       1 event.go:294] "Event occurred" object="default/frontend-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set frontend-deployment-848857f9c8 to 3"
I1019 04:54:28.076223       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ingress-nginx-controller-5959f988fd to 1"
I1019 04:54:28.076409       1 event.go:294] "Event occurred" object="default/flask-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set flask-deployment-d5d4d9df6 to 3"
I1019 04:54:28.085325       1 event.go:294] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set coredns-565d847f94 to 1"
I1019 04:54:28.175151       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1019 04:54:28.176356       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-admission-patch" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-admission-patch-fn6rt"
I1019 04:54:28.180584       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1019 04:54:28.183289       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1019 04:54:28.187112       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-admission-create" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-admission-create-nv98x"
I1019 04:54:28.195215       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1019 04:54:28.198618       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1019 04:54:28.201538       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1019 04:54:28.218158       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1019 04:54:28.227063       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1019 04:54:28.229785       1 event.go:294] "Event occurred" object="kube-system/kube-proxy" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kube-proxy-42s8s"
I1019 04:54:28.398975       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller-5959f988fd" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-controller-5959f988fd-45kbr"
I1019 04:54:28.399407       1 event.go:294] "Event occurred" object="kube-system/coredns-565d847f94" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: coredns-565d847f94-rvtjd"
I1019 04:54:28.413211       1 event.go:294] "Event occurred" object="default/flask-deployment-d5d4d9df6" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: flask-deployment-d5d4d9df6-6nwwc"
I1019 04:54:28.431044       1 event.go:294] "Event occurred" object="default/flask-deployment-d5d4d9df6" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: flask-deployment-d5d4d9df6-rrgrj"
I1019 04:54:28.436240       1 event.go:294] "Event occurred" object="default/flask-deployment-d5d4d9df6" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: flask-deployment-d5d4d9df6-2v5pf"
I1019 04:54:28.460885       1 event.go:294] "Event occurred" object="default/frontend-deployment-848857f9c8" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: frontend-deployment-848857f9c8-nc4qp"
I1019 04:54:28.501217       1 event.go:294] "Event occurred" object="default/frontend-deployment-848857f9c8" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: frontend-deployment-848857f9c8-xtxfs"
I1019 04:54:28.502238       1 event.go:294] "Event occurred" object="default/frontend-deployment-848857f9c8" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: frontend-deployment-848857f9c8-n2jfb"
I1019 04:54:38.690707       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1019 04:54:38.707348       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1019 04:54:39.840710       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1019 04:54:40.865106       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1019 04:54:40.874568       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1019 04:54:41.879655       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1019 04:54:41.896814       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1019 04:54:41.901262       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1019 04:54:41.902767       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-admission-create" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"
I1019 04:54:41.914526       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-create
E1019 04:54:41.920541       1 job_controller.go:545] syncing job: tracking status: adding uncounted pods to status: Operation cannot be fulfilled on jobs.batch "ingress-nginx-admission-create": the object has been modified; please apply your changes to the latest version and try again
I1019 04:54:42.904294       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1019 04:54:42.911203       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1019 04:54:42.912092       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-admission-patch" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"
I1019 04:54:42.916795       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-patch
E1019 04:54:42.920271       1 job_controller.go:545] syncing job: tracking status: adding uncounted pods to status: Operation cannot be fulfilled on jobs.batch "ingress-nginx-admission-patch": the object has been modified; please apply your changes to the latest version and try again

* 
* ==> kube-proxy [32e99e79a509] <==
* I1019 04:54:32.790505       1 node.go:163] Successfully retrieved node IP: 192.168.49.2
I1019 04:54:32.790592       1 server_others.go:138] "Detected node IP" address="192.168.49.2"
I1019 04:54:32.790610       1 server_others.go:578] "Unknown proxy mode, assuming iptables proxy" proxyMode=""
I1019 04:54:32.883833       1 server_others.go:206] "Using iptables Proxier"
I1019 04:54:32.884054       1 server_others.go:213] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I1019 04:54:32.884146       1 server_others.go:214] "Creating dualStackProxier for iptables"
I1019 04:54:32.884324       1 server_others.go:501] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6"
I1019 04:54:32.886016       1 proxier.go:262] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I1019 04:54:32.886791       1 proxier.go:262] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I1019 04:54:32.887280       1 server.go:661] "Version info" version="v1.25.0"
I1019 04:54:32.887315       1 server.go:663] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1019 04:54:32.888389       1 config.go:317] "Starting service config controller"
I1019 04:54:32.888474       1 shared_informer.go:255] Waiting for caches to sync for service config
I1019 04:54:32.888613       1 config.go:226] "Starting endpoint slice config controller"
I1019 04:54:32.888664       1 shared_informer.go:255] Waiting for caches to sync for endpoint slice config
I1019 04:54:32.889807       1 config.go:444] "Starting node config controller"
I1019 04:54:32.889843       1 shared_informer.go:255] Waiting for caches to sync for node config
I1019 04:54:32.988712       1 shared_informer.go:262] Caches are synced for service config
I1019 04:54:32.989877       1 shared_informer.go:262] Caches are synced for node config
I1019 04:54:32.989892       1 shared_informer.go:262] Caches are synced for endpoint slice config

* 
* ==> kube-scheduler [fab5befbe11e] <==
* I1019 04:54:08.397730       1 serving.go:348] Generated self-signed cert in-memory
W1019 04:54:11.580153       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1019 04:54:11.581287       1 authentication.go:346] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1019 04:54:11.581419       1 authentication.go:347] Continuing without authentication configuration. This may treat all requests as anonymous.
W1019 04:54:11.581654       1 authentication.go:348] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1019 04:54:11.615976       1 server.go:148] "Starting Kubernetes Scheduler" version="v1.25.0"
I1019 04:54:11.616019       1 server.go:150] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1019 04:54:11.623212       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1019 04:54:11.623248       1 shared_informer.go:255] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1019 04:54:11.623623       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I1019 04:54:11.623718       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
W1019 04:54:11.634013       1 reflector.go:424] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1019 04:54:11.635711       1 reflector.go:140] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W1019 04:54:11.636055       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1019 04:54:11.636160       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W1019 04:54:11.636455       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1019 04:54:11.636564       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W1019 04:54:11.637642       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1019 04:54:11.637682       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W1019 04:54:11.638234       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1019 04:54:11.638333       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W1019 04:54:11.638763       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1019 04:54:11.638866       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W1019 04:54:11.639054       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1019 04:54:11.639171       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W1019 04:54:11.639364       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1019 04:54:11.639484       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W1019 04:54:11.639617       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1019 04:54:11.639762       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W1019 04:54:11.639935       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1019 04:54:11.640075       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W1019 04:54:11.640240       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1019 04:54:11.640302       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W1019 04:54:11.640509       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1019 04:54:11.640927       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W1019 04:54:11.641948       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1019 04:54:11.642197       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W1019 04:54:11.642372       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1019 04:54:11.642930       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W1019 04:54:11.646465       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1019 04:54:11.646509       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W1019 04:54:12.504446       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1019 04:54:12.504805       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W1019 04:54:12.515354       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1019 04:54:12.515519       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W1019 04:54:12.524613       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1019 04:54:12.524751       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W1019 04:54:12.565374       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1019 04:54:12.565542       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W1019 04:54:12.581259       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1019 04:54:12.581308       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W1019 04:54:12.697329       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1019 04:54:12.697425       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W1019 04:54:12.707602       1 reflector.go:424] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1019 04:54:12.707643       1 reflector.go:140] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W1019 04:54:12.774605       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1019 04:54:12.774651       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
I1019 04:54:14.723868       1 shared_informer.go:262] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kubelet <==
* -- Logs begin at Wed 2022-10-19 04:53:40 UTC, end at Wed 2022-10-19 05:12:10 UTC. --
Oct 19 04:54:28 minikube kubelet[1772]: I1019 04:54:28.464091    1772 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-qwp88\" (UniqueName: \"kubernetes.io/projected/8bf790ac-6159-417d-bca7-f8d3db11a1ea-kube-api-access-qwp88\") pod \"flask-deployment-d5d4d9df6-6nwwc\" (UID: \"8bf790ac-6159-417d-bca7-f8d3db11a1ea\") " pod="default/flask-deployment-d5d4d9df6-6nwwc"
Oct 19 04:54:28 minikube kubelet[1772]: I1019 04:54:28.464263    1772 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/c8029e2b-ef11-4b53-9ac7-42ec19afcf7d-webhook-cert\") pod \"ingress-nginx-controller-5959f988fd-45kbr\" (UID: \"c8029e2b-ef11-4b53-9ac7-42ec19afcf7d\") " pod="ingress-nginx/ingress-nginx-controller-5959f988fd-45kbr"
Oct 19 04:54:28 minikube kubelet[1772]: I1019 04:54:28.464450    1772 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-vl5vn\" (UniqueName: \"kubernetes.io/projected/72a6184d-0f90-43a2-996e-9f8eb0e70b7c-kube-api-access-vl5vn\") pod \"coredns-565d847f94-rvtjd\" (UID: \"72a6184d-0f90-43a2-996e-9f8eb0e70b7c\") " pod="kube-system/coredns-565d847f94-rvtjd"
Oct 19 04:54:28 minikube kubelet[1772]: I1019 04:54:28.464515    1772 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-vkrkq\" (UniqueName: \"kubernetes.io/projected/c8029e2b-ef11-4b53-9ac7-42ec19afcf7d-kube-api-access-vkrkq\") pod \"ingress-nginx-controller-5959f988fd-45kbr\" (UID: \"c8029e2b-ef11-4b53-9ac7-42ec19afcf7d\") " pod="ingress-nginx/ingress-nginx-controller-5959f988fd-45kbr"
Oct 19 04:54:28 minikube kubelet[1772]: I1019 04:54:28.467327    1772 topology_manager.go:205] "Topology Admit Handler"
Oct 19 04:54:28 minikube kubelet[1772]: I1019 04:54:28.468456    1772 topology_manager.go:205] "Topology Admit Handler"
Oct 19 04:54:28 minikube kubelet[1772]: I1019 04:54:28.470213    1772 topology_manager.go:205] "Topology Admit Handler"
Oct 19 04:54:28 minikube kubelet[1772]: I1019 04:54:28.514922    1772 topology_manager.go:205] "Topology Admit Handler"
Oct 19 04:54:28 minikube kubelet[1772]: I1019 04:54:28.524083    1772 topology_manager.go:205] "Topology Admit Handler"
Oct 19 04:54:28 minikube kubelet[1772]: I1019 04:54:28.564920    1772 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-lq7wd\" (UniqueName: \"kubernetes.io/projected/99478401-b27f-4e2b-a490-37090dfbbeb8-kube-api-access-lq7wd\") pod \"flask-deployment-d5d4d9df6-rrgrj\" (UID: \"99478401-b27f-4e2b-a490-37090dfbbeb8\") " pod="default/flask-deployment-d5d4d9df6-rrgrj"
Oct 19 04:54:28 minikube kubelet[1772]: I1019 04:54:28.565038    1772 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-4wnk4\" (UniqueName: \"kubernetes.io/projected/5bcd7cd7-c8a5-4367-8a20-cec445363b57-kube-api-access-4wnk4\") pod \"frontend-deployment-848857f9c8-xtxfs\" (UID: \"5bcd7cd7-c8a5-4367-8a20-cec445363b57\") " pod="default/frontend-deployment-848857f9c8-xtxfs"
Oct 19 04:54:28 minikube kubelet[1772]: I1019 04:54:28.565079    1772 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-mm9rl\" (UniqueName: \"kubernetes.io/projected/3e5cb70a-993b-444c-ac3d-93d6e80410c5-kube-api-access-mm9rl\") pod \"frontend-deployment-848857f9c8-nc4qp\" (UID: \"3e5cb70a-993b-444c-ac3d-93d6e80410c5\") " pod="default/frontend-deployment-848857f9c8-nc4qp"
Oct 19 04:54:28 minikube kubelet[1772]: I1019 04:54:28.565135    1772 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-rl6jh\" (UniqueName: \"kubernetes.io/projected/d4715eef-d39c-4497-9cb2-efc9eb9652bd-kube-api-access-rl6jh\") pod \"flask-deployment-d5d4d9df6-2v5pf\" (UID: \"d4715eef-d39c-4497-9cb2-efc9eb9652bd\") " pod="default/flask-deployment-d5d4d9df6-2v5pf"
Oct 19 04:54:28 minikube kubelet[1772]: I1019 04:54:28.565227    1772 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-2f865\" (UniqueName: \"kubernetes.io/projected/d8f0d4c0-c975-48e0-8e09-4835870c5e4f-kube-api-access-2f865\") pod \"frontend-deployment-848857f9c8-n2jfb\" (UID: \"d8f0d4c0-c975-48e0-8e09-4835870c5e4f\") " pod="default/frontend-deployment-848857f9c8-n2jfb"
Oct 19 04:54:28 minikube kubelet[1772]: E1019 04:54:28.657824    1772 projected.go:290] Couldn't get configMap kube-system/kube-root-ca.crt: failed to sync configmap cache: timed out waiting for the condition
Oct 19 04:54:28 minikube kubelet[1772]: E1019 04:54:28.657940    1772 projected.go:196] Error preparing data for projected volume kube-api-access-vzls2 for pod kube-system/storage-provisioner: failed to sync configmap cache: timed out waiting for the condition
Oct 19 04:54:28 minikube kubelet[1772]: E1019 04:54:28.658095    1772 nestedpendingoperations.go:335] Operation for "{volumeName:kubernetes.io/projected/67d9244a-4b25-433b-b2a2-59cd9c4ce401-kube-api-access-vzls2 podName:67d9244a-4b25-433b-b2a2-59cd9c4ce401 nodeName:}" failed. No retries permitted until 2022-10-19 04:54:29.15799272 +0000 UTC m=+14.544079831 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "kube-api-access-vzls2" (UniqueName: "kubernetes.io/projected/67d9244a-4b25-433b-b2a2-59cd9c4ce401-kube-api-access-vzls2") pod "storage-provisioner" (UID: "67d9244a-4b25-433b-b2a2-59cd9c4ce401") : failed to sync configmap cache: timed out waiting for the condition
Oct 19 04:54:28 minikube kubelet[1772]: E1019 04:54:28.686775    1772 secret.go:192] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Oct 19 04:54:28 minikube kubelet[1772]: E1019 04:54:28.686853    1772 nestedpendingoperations.go:335] Operation for "{volumeName:kubernetes.io/secret/c8029e2b-ef11-4b53-9ac7-42ec19afcf7d-webhook-cert podName:c8029e2b-ef11-4b53-9ac7-42ec19afcf7d nodeName:}" failed. No retries permitted until 2022-10-19 04:54:29.186838644 +0000 UTC m=+14.572925748 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/c8029e2b-ef11-4b53-9ac7-42ec19afcf7d-webhook-cert") pod "ingress-nginx-controller-5959f988fd-45kbr" (UID: "c8029e2b-ef11-4b53-9ac7-42ec19afcf7d") : secret "ingress-nginx-admission" not found
Oct 19 04:54:29 minikube kubelet[1772]: E1019 04:54:29.274159    1772 secret.go:192] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Oct 19 04:54:29 minikube kubelet[1772]: E1019 04:54:29.274438    1772 nestedpendingoperations.go:335] Operation for "{volumeName:kubernetes.io/secret/c8029e2b-ef11-4b53-9ac7-42ec19afcf7d-webhook-cert podName:c8029e2b-ef11-4b53-9ac7-42ec19afcf7d nodeName:}" failed. No retries permitted until 2022-10-19 04:54:30.274414286 +0000 UTC m=+15.660501395 (durationBeforeRetry 1s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/c8029e2b-ef11-4b53-9ac7-42ec19afcf7d-webhook-cert") pod "ingress-nginx-controller-5959f988fd-45kbr" (UID: "c8029e2b-ef11-4b53-9ac7-42ec19afcf7d") : secret "ingress-nginx-admission" not found
Oct 19 04:54:29 minikube kubelet[1772]: E1019 04:54:29.369516    1772 projected.go:290] Couldn't get configMap kube-system/kube-root-ca.crt: failed to sync configmap cache: timed out waiting for the condition
Oct 19 04:54:29 minikube kubelet[1772]: E1019 04:54:29.369765    1772 projected.go:196] Error preparing data for projected volume kube-api-access-fn27p for pod kube-system/kube-proxy-42s8s: failed to sync configmap cache: timed out waiting for the condition
Oct 19 04:54:29 minikube kubelet[1772]: E1019 04:54:29.369920    1772 nestedpendingoperations.go:335] Operation for "{volumeName:kubernetes.io/projected/1151e344-4c57-4d40-b741-3ca0febef73c-kube-api-access-fn27p podName:1151e344-4c57-4d40-b741-3ca0febef73c nodeName:}" failed. No retries permitted until 2022-10-19 04:54:29.86989663 +0000 UTC m=+15.255983738 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "kube-api-access-fn27p" (UniqueName: "kubernetes.io/projected/1151e344-4c57-4d40-b741-3ca0febef73c-kube-api-access-fn27p") pod "kube-proxy-42s8s" (UID: "1151e344-4c57-4d40-b741-3ca0febef73c") : failed to sync configmap cache: timed out waiting for the condition
Oct 19 04:54:29 minikube kubelet[1772]: I1019 04:54:29.677789    1772 request.go:601] Waited for 1.112183928s due to client-side throttling, not priority and fairness, request: POST:https://control-plane.minikube.internal:8443/api/v1/namespaces/ingress-nginx/serviceaccounts/ingress-nginx/token
Oct 19 04:54:30 minikube kubelet[1772]: E1019 04:54:30.173217    1772 projected.go:290] Couldn't get configMap kube-system/kube-root-ca.crt: failed to sync configmap cache: timed out waiting for the condition
Oct 19 04:54:30 minikube kubelet[1772]: E1019 04:54:30.173293    1772 projected.go:196] Error preparing data for projected volume kube-api-access-vzls2 for pod kube-system/storage-provisioner: failed to sync configmap cache: timed out waiting for the condition
Oct 19 04:54:30 minikube kubelet[1772]: E1019 04:54:30.173552    1772 nestedpendingoperations.go:335] Operation for "{volumeName:kubernetes.io/projected/67d9244a-4b25-433b-b2a2-59cd9c4ce401-kube-api-access-vzls2 podName:67d9244a-4b25-433b-b2a2-59cd9c4ce401 nodeName:}" failed. No retries permitted until 2022-10-19 04:54:31.173504328 +0000 UTC m=+16.559591439 (durationBeforeRetry 1s). Error: MountVolume.SetUp failed for volume "kube-api-access-vzls2" (UniqueName: "kubernetes.io/projected/67d9244a-4b25-433b-b2a2-59cd9c4ce401-kube-api-access-vzls2") pod "storage-provisioner" (UID: "67d9244a-4b25-433b-b2a2-59cd9c4ce401") : failed to sync configmap cache: timed out waiting for the condition
Oct 19 04:54:30 minikube kubelet[1772]: E1019 04:54:30.295755    1772 secret.go:192] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Oct 19 04:54:30 minikube kubelet[1772]: E1019 04:54:30.295837    1772 nestedpendingoperations.go:335] Operation for "{volumeName:kubernetes.io/secret/c8029e2b-ef11-4b53-9ac7-42ec19afcf7d-webhook-cert podName:c8029e2b-ef11-4b53-9ac7-42ec19afcf7d nodeName:}" failed. No retries permitted until 2022-10-19 04:54:32.295820547 +0000 UTC m=+17.681907654 (durationBeforeRetry 2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/c8029e2b-ef11-4b53-9ac7-42ec19afcf7d-webhook-cert") pod "ingress-nginx-controller-5959f988fd-45kbr" (UID: "c8029e2b-ef11-4b53-9ac7-42ec19afcf7d") : secret "ingress-nginx-admission" not found
Oct 19 04:54:30 minikube kubelet[1772]: E1019 04:54:30.524901    1772 projected.go:290] Couldn't get configMap kube-system/kube-root-ca.crt: failed to sync configmap cache: timed out waiting for the condition
Oct 19 04:54:30 minikube kubelet[1772]: E1019 04:54:30.524931    1772 projected.go:196] Error preparing data for projected volume kube-api-access-vl5vn for pod kube-system/coredns-565d847f94-rvtjd: failed to sync configmap cache: timed out waiting for the condition
Oct 19 04:54:30 minikube kubelet[1772]: E1019 04:54:30.525005    1772 nestedpendingoperations.go:335] Operation for "{volumeName:kubernetes.io/projected/72a6184d-0f90-43a2-996e-9f8eb0e70b7c-kube-api-access-vl5vn podName:72a6184d-0f90-43a2-996e-9f8eb0e70b7c nodeName:}" failed. No retries permitted until 2022-10-19 04:54:31.024989898 +0000 UTC m=+16.411077005 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "kube-api-access-vl5vn" (UniqueName: "kubernetes.io/projected/72a6184d-0f90-43a2-996e-9f8eb0e70b7c-kube-api-access-vl5vn") pod "coredns-565d847f94-rvtjd" (UID: "72a6184d-0f90-43a2-996e-9f8eb0e70b7c") : failed to sync configmap cache: timed out waiting for the condition
Oct 19 04:54:30 minikube kubelet[1772]: E1019 04:54:30.890356    1772 projected.go:290] Couldn't get configMap kube-system/kube-root-ca.crt: failed to sync configmap cache: timed out waiting for the condition
Oct 19 04:54:30 minikube kubelet[1772]: E1019 04:54:30.890382    1772 projected.go:196] Error preparing data for projected volume kube-api-access-fn27p for pod kube-system/kube-proxy-42s8s: failed to sync configmap cache: timed out waiting for the condition
Oct 19 04:54:30 minikube kubelet[1772]: E1019 04:54:30.890457    1772 nestedpendingoperations.go:335] Operation for "{volumeName:kubernetes.io/projected/1151e344-4c57-4d40-b741-3ca0febef73c-kube-api-access-fn27p podName:1151e344-4c57-4d40-b741-3ca0febef73c nodeName:}" failed. No retries permitted until 2022-10-19 04:54:31.890443294 +0000 UTC m=+17.276530402 (durationBeforeRetry 1s). Error: MountVolume.SetUp failed for volume "kube-api-access-fn27p" (UniqueName: "kubernetes.io/projected/1151e344-4c57-4d40-b741-3ca0febef73c-kube-api-access-fn27p") pod "kube-proxy-42s8s" (UID: "1151e344-4c57-4d40-b741-3ca0febef73c") : failed to sync configmap cache: timed out waiting for the condition
Oct 19 04:54:30 minikube kubelet[1772]: I1019 04:54:30.898580    1772 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="88c16f539e3d06349f4e12cb9a17d76c166d37f96efd055f0aa6a1030eb83562"
Oct 19 04:54:32 minikube kubelet[1772]: I1019 04:54:32.005225    1772 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="966e445317e2de35aa7ce34703fd86a2826310949e603ecabf5a3adf007c346c"
Oct 19 04:54:32 minikube kubelet[1772]: I1019 04:54:32.187472    1772 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="12f63d99e4d72ba9ba9b593f7713fb408c52d14d8493e74bab5ad96d447f6ab3"
Oct 19 04:54:32 minikube kubelet[1772]: E1019 04:54:32.338741    1772 secret.go:192] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Oct 19 04:54:32 minikube kubelet[1772]: E1019 04:54:32.338831    1772 nestedpendingoperations.go:335] Operation for "{volumeName:kubernetes.io/secret/c8029e2b-ef11-4b53-9ac7-42ec19afcf7d-webhook-cert podName:c8029e2b-ef11-4b53-9ac7-42ec19afcf7d nodeName:}" failed. No retries permitted until 2022-10-19 04:54:36.338815247 +0000 UTC m=+21.724902351 (durationBeforeRetry 4s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/c8029e2b-ef11-4b53-9ac7-42ec19afcf7d-webhook-cert") pod "ingress-nginx-controller-5959f988fd-45kbr" (UID: "c8029e2b-ef11-4b53-9ac7-42ec19afcf7d") : secret "ingress-nginx-admission" not found
Oct 19 04:54:32 minikube kubelet[1772]: I1019 04:54:32.393432    1772 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="36743ce6b7b8efeeb70c8f32e355b72262de03447a52789f8ca96237569aa2d6"
Oct 19 04:54:32 minikube kubelet[1772]: I1019 04:54:32.419661    1772 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="b2f22dfb07d409a49ed41a9f4ec5e1fb6e3d0539ffef7f1309b84fef9deb1d08"
Oct 19 04:54:36 minikube kubelet[1772]: E1019 04:54:36.374210    1772 secret.go:192] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Oct 19 04:54:36 minikube kubelet[1772]: E1019 04:54:36.374309    1772 nestedpendingoperations.go:335] Operation for "{volumeName:kubernetes.io/secret/c8029e2b-ef11-4b53-9ac7-42ec19afcf7d-webhook-cert podName:c8029e2b-ef11-4b53-9ac7-42ec19afcf7d nodeName:}" failed. No retries permitted until 2022-10-19 04:54:44.374293833 +0000 UTC m=+29.760380944 (durationBeforeRetry 8s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/c8029e2b-ef11-4b53-9ac7-42ec19afcf7d-webhook-cert") pod "ingress-nginx-controller-5959f988fd-45kbr" (UID: "c8029e2b-ef11-4b53-9ac7-42ec19afcf7d") : secret "ingress-nginx-admission" not found
Oct 19 04:54:38 minikube kubelet[1772]: I1019 04:54:38.696135    1772 scope.go:115] "RemoveContainer" containerID="5121e22ca47e92f985f55037be3c22deddb95459ed1a6a5cdf87d62ed4cce054"
Oct 19 04:54:39 minikube kubelet[1772]: I1019 04:54:39.829778    1772 scope.go:115] "RemoveContainer" containerID="5121e22ca47e92f985f55037be3c22deddb95459ed1a6a5cdf87d62ed4cce054"
Oct 19 04:54:40 minikube kubelet[1772]: I1019 04:54:40.111401    1772 reconciler.go:211] "operationExecutor.UnmountVolume started for volume \"kube-api-access-zn7xt\" (UniqueName: \"kubernetes.io/projected/d9b074bf-5d74-406f-bdb7-2bb68c4cfe31-kube-api-access-zn7xt\") pod \"d9b074bf-5d74-406f-bdb7-2bb68c4cfe31\" (UID: \"d9b074bf-5d74-406f-bdb7-2bb68c4cfe31\") "
Oct 19 04:54:40 minikube kubelet[1772]: I1019 04:54:40.120406    1772 operation_generator.go:890] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/d9b074bf-5d74-406f-bdb7-2bb68c4cfe31-kube-api-access-zn7xt" (OuterVolumeSpecName: "kube-api-access-zn7xt") pod "d9b074bf-5d74-406f-bdb7-2bb68c4cfe31" (UID: "d9b074bf-5d74-406f-bdb7-2bb68c4cfe31"). InnerVolumeSpecName "kube-api-access-zn7xt". PluginName "kubernetes.io/projected", VolumeGidValue ""
Oct 19 04:54:40 minikube kubelet[1772]: I1019 04:54:40.212269    1772 reconciler.go:399] "Volume detached for volume \"kube-api-access-zn7xt\" (UniqueName: \"kubernetes.io/projected/d9b074bf-5d74-406f-bdb7-2bb68c4cfe31-kube-api-access-zn7xt\") on node \"minikube\" DevicePath \"\""
Oct 19 04:54:40 minikube kubelet[1772]: I1019 04:54:40.846801    1772 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="aecaae456ae8384ea3a45cb938662350b177aa737727db9bcc252dfa39505f7d"
Oct 19 04:54:41 minikube kubelet[1772]: I1019 04:54:41.019325    1772 reconciler.go:211] "operationExecutor.UnmountVolume started for volume \"kube-api-access-4qmzb\" (UniqueName: \"kubernetes.io/projected/87881d87-e035-4c2f-9111-3981d71805af-kube-api-access-4qmzb\") pod \"87881d87-e035-4c2f-9111-3981d71805af\" (UID: \"87881d87-e035-4c2f-9111-3981d71805af\") "
Oct 19 04:54:41 minikube kubelet[1772]: I1019 04:54:41.023348    1772 operation_generator.go:890] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/87881d87-e035-4c2f-9111-3981d71805af-kube-api-access-4qmzb" (OuterVolumeSpecName: "kube-api-access-4qmzb") pod "87881d87-e035-4c2f-9111-3981d71805af" (UID: "87881d87-e035-4c2f-9111-3981d71805af"). InnerVolumeSpecName "kube-api-access-4qmzb". PluginName "kubernetes.io/projected", VolumeGidValue ""
Oct 19 04:54:41 minikube kubelet[1772]: I1019 04:54:41.120266    1772 reconciler.go:399] "Volume detached for volume \"kube-api-access-4qmzb\" (UniqueName: \"kubernetes.io/projected/87881d87-e035-4c2f-9111-3981d71805af-kube-api-access-4qmzb\") on node \"minikube\" DevicePath \"\""
Oct 19 04:54:41 minikube kubelet[1772]: I1019 04:54:41.882416    1772 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="6e9427276b29dc7b98e87c68acd7ef8b11f6cc01ab655dc6be2d001ca1a55e69"
Oct 19 04:54:45 minikube kubelet[1772]: I1019 04:54:45.006436    1772 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="c4ed66d1780a4e72c383932ae08992da4df7c35bb9d36128038cb290f21f7e42"
Oct 19 04:55:03 minikube kubelet[1772]: I1019 04:55:03.201197    1772 scope.go:115] "RemoveContainer" containerID="69c68b0651b0f5c936de78a4fb6901d18d307d63820409c39806c50eee86e0df"
Oct 19 04:59:14 minikube kubelet[1772]: W1019 04:59:14.730957    1772 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Oct 19 05:04:14 minikube kubelet[1772]: W1019 05:04:14.510250    1772 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Oct 19 05:09:14 minikube kubelet[1772]: W1019 05:09:14.278296    1772 sysinfo.go:203] Nodes topology is not available, providing CPU topology

* 
* ==> storage-provisioner [670ca3b75e22] <==
* I1019 04:55:03.432609       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1019 04:55:03.443869       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1019 04:55:03.444015       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1019 04:55:03.455963       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1019 04:55:03.456631       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"2d39d2e2-b17d-4137-9cf9-c1d71f878d97", APIVersion:"v1", ResourceVersion:"539", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_a38da8ca-ade0-4df3-aa83-5109b3f3fda8 became leader
I1019 04:55:03.456828       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_a38da8ca-ade0-4df3-aa83-5109b3f3fda8!
I1019 04:55:03.557286       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_a38da8ca-ade0-4df3-aa83-5109b3f3fda8!

* 
* ==> storage-provisioner [69c68b0651b0] <==
* I1019 04:54:32.164352       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1019 04:55:02.147102       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

